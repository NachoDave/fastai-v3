{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from exp.nb_09 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('boom_doom_wee_dg', re.compile(r'(.)([A-Z][a-z]+)', re.UNICODE))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
    "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
    "\n",
    "# Converts camel names to snake names\n",
    "def camel2snake(name):\n",
    "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
    "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
    "\n",
    "c2s = camel2snake(\"BoomDoomWeeDg\")\n",
    "c2s, _camel_re1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Exceptions as flow control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback():\n",
    "    _order=0\n",
    "    def set_runner(self, run): self.run=run\n",
    "    def __getattr__(self, k): return getattr(self.run, k) # if cannot find the attribute inside the cb, \n",
    "    #look inside the runner\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')\n",
    "    \n",
    "    def __call__(self, cb_name): \n",
    "        f = getattr(self, cb_name, None) # pass the call back def name to get attr to run the function\n",
    "        if f and f(): return True\n",
    "        return False\n",
    "\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs=0.\n",
    "        self.run.n_iter=0\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter   += 1\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs=self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train=True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train=False\n",
    "\n",
    "class CancelTrainException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "\n",
    "class TestCbsCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        print(\"I'm a test callback\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Learner and runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def param_getter(m): return m.parameters() #  getst the parameters of a model\n",
    "\n",
    "def sgd_opt():\n",
    "    pass\n",
    "\n",
    "class LearnerRunnerDJ():\n",
    "    \n",
    "    def __init__(self, model, data, loss_func, opt_func=sgd_opt, lr=1e-2, splitter=param_getter,\n",
    "                 cbs=None, cb_funcs=None): # callback functions used to create callbacks\n",
    "        \n",
    "        # model - the model\n",
    "        # data \n",
    "        # loss func - usually cross entropy\n",
    "        # opt func - optimiser\n",
    "        self.model , self.data, self.loss_func, self.opt_func, self.lr, self.splitter = model, data, loss_func, opt_func,lr, splitter\n",
    "        \n",
    "        self.logger, self.in_train, self.opt = print, False, None # the logger is a print function to print the output from a nn\n",
    "        \n",
    "        self.cbs = []\n",
    "        self.add_cb(TrainEvalCallback())\n",
    "        self.add_cbs(cbs)\n",
    "        self.add_cbs(cbf() for cbf in listify(cb_funcs)) # goes through all callback creastor funcs and runs them\n",
    "        \n",
    "    def add_cbs(self, cbs):\n",
    "        for cb in listify(cbs): self.add_cb(cb) # call add cb on all callbacks\n",
    "            \n",
    "    def add_cb(self, cb):\n",
    "        cb.set_runner(self) # sets a 'pointer' to the runner inside the callback (so can access runner members such as)\n",
    "        #set_trace()\n",
    "        setattr(self, cb.name, cb) # set the callback as a member of the learner class\n",
    "        self.cbs.append(cb) # add the callback to the call backs list\n",
    "\n",
    "    def remove_cbs(self, cbs):\n",
    "        for cb in listify(cbs): self.cbs.remove(cb)\n",
    "            \n",
    "    def all_batches(self):\n",
    "        self.iters = len(self.dl) # Batch size is defined by the data loader\n",
    "        try:\n",
    "            for i, (xb, yb) in enumerate(self.dl): # gets some data from the dataloader\n",
    "                self.one_batch(i, xb, yb)\n",
    "        except CancelEpochException: self('after_cancel_batch')\n",
    "            \n",
    "    \n",
    "    def one_batch(self, i, xb, yb): # all the stuff\n",
    "        try:\n",
    "            self.iter = i\n",
    "            self.xb, self.yb = xb, yb;                      self('begin_batch')\n",
    "            # Run model\n",
    "            self.pred = self.model(xb);                     self('after_pred')\n",
    "            # Calculate loss\n",
    "            self.loss = self.loss_func(self.yb, self.pred); self('after_loss')\n",
    "            # Update parameters if in \n",
    "            if not self.in_train: return\n",
    "            # perform backward pass calculates gradients\n",
    "            self.loss.backward();                           self('after_backward')\n",
    "            # perform step (updates parameter)         \n",
    "            self.opt.step();                                self('after_step')\n",
    "            # zero gradients\n",
    "            self.opt.zero_grad();                           \n",
    "        except CancelBatchException:                        self('cancel_batch')\n",
    "        finally: self('after_batch')\n",
    " \n",
    "    # Function to begin a fit\n",
    "    def do_begin_fit(self, epochs):\n",
    "        self.epochs,self.loss = epochs,tensor(0.)\n",
    "        self('begin_fit')\n",
    "        \n",
    "    # Method to begin an epoch\n",
    "    def do_begin_epoch(self, epoch):\n",
    "        self.dl, self.epoch = self.data.train_dl, epoch # set the current data loader to training\n",
    "        print(\"In do_begin_epoch\")\n",
    "        return self('begin_epoch')\n",
    "    \n",
    "    def fit(self, epochs, cbs=None, reset_opt=False): # main fitting function\n",
    "        # can now pass in additional callbacks to fit\n",
    "        self.add_cbs(cbs)\n",
    "        #create optimizer on fit(), optionally replacing existing\n",
    "        #if reset_opt or not self.opt: self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n",
    "            \n",
    "        # try running\n",
    "        try:\n",
    "            self.do_begin_fit(epochs)\n",
    "            # Run all training batches for the epoch - runs through all training data \n",
    "            for epoch in range(epochs):\n",
    "                self.do_begin_epoch(epoch)\n",
    "                if not self('begin_epoch'): self.all_batches()\n",
    "                \n",
    "            \n",
    "                # Run all validation batches for the epoch -runs through all validation data\n",
    "                with torch.no_grad(): # turn off gradients\n",
    "                    self.dl = self.data.valid_dl # set current data loader to validation\n",
    "                    if not self('begin_validate'): self.all_batches()\n",
    "                \n",
    "            self('after_epoch')\n",
    "       # Use exceptions to cance training\n",
    "        except CancelTrainException: self('after_cancel_train')\n",
    "        # s\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.remove_cbs(cbs)\n",
    "            \n",
    "    ALL_CBS = {'begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step',\n",
    "        'after_cancel_batch', 'after_batch', 'after_cancel_epoch', 'begin_fit',\n",
    "        'begin_epoch', 'begin_validate', 'after_epoch',\n",
    "        'after_cancel_train', 'after_fit'}\n",
    "        \n",
    "    def __call__(self, cb_name):\n",
    "        res = False\n",
    "        assert cb_name in self.ALL_CBS\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n",
    "        #set_trace()\n",
    "        return res # callback needs to return True to stop. With no return Python returns None (False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average stats call back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgStats(): # average stats is a class to calculate and store training stats (i.e accuracy)\n",
    "    def __init__(self, metrics, in_train): \n",
    "        self.metrics,self.in_train = listify(metrics),in_train\n",
    "        print(\"Metrics:\")\n",
    "        print(metrics)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.tot_loss,self.count = 0.,0\n",
    "        self.tot_mets = [0.] * len(self.metrics)\n",
    "        \n",
    "    @property\n",
    "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
    "    @property\n",
    "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if not self.count: return \"\"\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, run): # run is presumably the runner\n",
    "        bn = run.xb.shape[0] # xb is the mini batch size\n",
    "        self.tot_loss += run.loss * bn # I think this is accounting for the batch size\n",
    "        self.count += bn\n",
    "        for i,m in enumerate(self.metrics): # loops through metric functions and applies them to predictions and y\n",
    "            self.tot_mets[i] += m(run.pred, run.yb) * bn # can have any metrics on the predictions and truth \n",
    "\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        # train_stats and valid_stats are containers for stats for training and valdation sets\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        \n",
    "    def after_loss(self): # after loss, the accumulates the loss on each mini batch\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        #We use the logger function of the `Learner` here, it can be customized to write in a file or in a progress bar\n",
    "        self.logger(self.train_stats)\n",
    "        self.logger(self.valid_stats) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading 1) Image list containers\n",
    "These objects store a list of image locations. Image can be accessed by index or iterator. Asking for a single image opens the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files, function to get the image files stored in a directory structure\n",
    "#export\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res\n",
    "#export\n",
    "def get_files(path, extensions=None, recurse=False, include=None):\n",
    "    path = Path(path)\n",
    "    extensions = setify(extensions)\n",
    "    extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "            if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "            else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "            res += _get_files(p, f, extensions)\n",
    "        return res\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        return _get_files(path, f, extensions)\n",
    "\n",
    "# Item list base class \n",
    "class ListContainer():\n",
    "    \n",
    "    def __init__(self, lst):\n",
    "        #assert(isinstance(lst, list))\n",
    "        self.lst = listify(lst)\n",
    "    \n",
    "    # different responses to accessing the item list, including ints, slices and \n",
    "    def __getitem__(self,idx):\n",
    "        if isinstance(idx, (int, slice)): return self.lst[idx]\n",
    "        if isinstance(idx[0], bool):\n",
    "            assert len(idx)==len(self)\n",
    "            return [o for dx,o in zip(idx, self.lst) if dx]\n",
    "        return [self.lst[i] for i in idx]\n",
    "        \n",
    "    # get length of item list\n",
    "    def __len__(self): return len(self.lst)\n",
    "    \n",
    "    # repr\n",
    "    def __repr__(self):\n",
    "        res = f'({self.__class__.__name__})({len(self)} items)\\n{self.lst[:10]}'\n",
    "        if len(self) > 100 : res = res + f'...'\n",
    "        return res\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.lst)\n",
    "    \n",
    "    def __setitem__(self, itm, dx = False):\n",
    "        \n",
    "        if dx:\n",
    "            self.lst[dx] = itm\n",
    "        else: self.lst.append(itm)\n",
    "            \n",
    "    def __delitem__(self, i):\n",
    "        \n",
    "        try:\n",
    "            assert(len(self) - 1 >= i)\n",
    "        \n",
    "            del(self.lst[i])\n",
    "        except AssertionError as error:\n",
    "            print(error, \" Deletion out of range of list\")\n",
    "\n",
    "# Listify - returns a list\n",
    "def listify(x):\n",
    "    if x is None: return []\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, str): return list(x)\n",
    "    if isinstance(x, set): return list(x)\n",
    "    return [x]\n",
    "\n",
    "# Compose function (takes a list of functions and applies them in order (giving the function an order attribute))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    #set_trace()\n",
    "    if funcs is None: return x\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): \n",
    "        #print(f)\n",
    "        x = f(x, **kwargs)\n",
    "    return x\n",
    "\n",
    "\n",
    "# List container base class\n",
    "# should store something in a list and return it in applying functions\n",
    "class ItemList(ListContainer):    \n",
    "    def __init__(self, lst, pth = '.', trnsfrms = None):\n",
    "        super().__init__(lst)\n",
    "        self.trnsfrms, self.pth = trnsfrms, pth\n",
    "        \n",
    "    def __repr__(self): \n",
    "        res = super().__repr__() + self.pth\n",
    "        return res\n",
    "        \n",
    "    def get(self, itm): return itm\n",
    "    def _get(self, itm):\n",
    "        return compose(self.get(itm), self.trnsfrms)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        itms = super().__getitem__(idx)\n",
    "        if isinstance(itms, list): return [self._get(o) for o in itms]\n",
    "        return self._get(itms)\n",
    "        \n",
    "    # This new allows the class to easily make a copy of itself\n",
    "    def new(self, items, cls=None):\n",
    "        if cls is None: cls=self.__class__\n",
    "        return cls(items, self.pth, trnsfrms=self.trnsfrms)\n",
    "    \n",
    "# This is an image container class. Uses PIL lirary to ope images\n",
    "class ImageList(ItemList):\n",
    "    def get(self, fn): return PIL.Image.open(fn)\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):\n",
    "        if extensions is None: extensions = image_extensions\n",
    "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader 2) Processor and splitter\n",
    "1) Processor labels the data so that we can use the same encoding the labels on different data sets\n",
    "\n",
    "2) Splitter applies the dataset into random and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueify (returns a list of the unique files)\n",
    "from collections import OrderedDict\n",
    "\n",
    "def uniqueify(x, sort=False):\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res\n",
    "\n",
    "# data Processor (assigns an integer refernce to each class in the dataset)\n",
    "\n",
    "class Processor():\n",
    "    def __init__(self, vocab = None):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __call__(self, items): # returns a list of the values corresponding to the classes\n",
    "        if self.vocab is None:\n",
    "            self.vocab = uniqueify(items)\n",
    "            self.otoi = {v:k for k, v in enumerate(self.vocab)} # makes a dictionary of numbers and key values\n",
    "        return [self.proc1(i)for i in items] # key is the class, i is the label integer for otoi, vocab is \n",
    "        # an ordered list of the possible classes with no label (same order as otoi)\n",
    "        \n",
    "    def proc1(self, item):  return self.otoi[item]\n",
    "    \n",
    "    def deProcess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs] # returns the classes from a list of indexes\n",
    "        \n",
    "    def deproc1(self, idx):\n",
    "        return self.vocab[idx]\n",
    "\n",
    "def parent_labeler(fn): return fn.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter stuff\n",
    "# Grand parent splitter splits based on the whether the grand parnt folder name is train or valid\n",
    "def grandparent_splitter(fn, valid_name='valid', train_name='train'):\n",
    "    gp = fn.parent.parent.name\n",
    "    return True if gp==valid_name else False if gp==train_name else None\n",
    "\n",
    "# Just creates a random number, should probably check item is an image\n",
    "def rand_splitter(fn, trainFrac = 0.7):\n",
    "    p = float(torch.rand(1))\n",
    "    return True if p > trainFrac else False\n",
    "\n",
    "\n",
    "\n",
    "# Applies the splitter function to all items in the item list and splits them into different groups\n",
    "# Puts the train set first\n",
    "def split_by_func(items, f):\n",
    "    mask = [f(o) for o in items]\n",
    "     #`None` values will be filtered out\n",
    "    f = [o for o,m in zip(items,mask) if m==False]\n",
    "    t = [o for o,m in zip(items,mask) if m==True ]\n",
    "    return f,t\n",
    "\n",
    "randSplitter = partial(split_by_func, f = rand_splitter)\n",
    "\n",
    "# class split data - splits and stores training and validation data\n",
    "class SplitData():\n",
    "    def __init__(self, train, valid):\n",
    "        self.train, self.valid = train, valid\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}\\nTrain: {self.train} \\n\\nValid: {self.valid}'\n",
    "\n",
    "    #This is needed if we want to pickle SplitData and be able to load it back without recursion errors\n",
    "    def __setstate__(self,data:Any): self.__dict__.update(data) \n",
    "    \n",
    "    @classmethod\n",
    "    def split_by_func(cls, il, f):\n",
    "        train, valid = map(il.new, split_by_func(il, f)) # map function creates iterator applying function to all elements of iterable\n",
    "        return cls(train, valid)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally - labelled data class stores labelled data for training and validation set. It stores the x and y (converted)\n",
    "# to integer labels) \n",
    "\n",
    "def _label_by_func(ds, f, cls=ItemList): return cls([f(o) for o in ds.lst], pth=ds.pth)\n",
    "\n",
    "class LabelledData():\n",
    "    def process(self, il, proc): \n",
    "        #set_trace()\n",
    "        return il.new(compose(il.lst, proc))\n",
    "\n",
    "    def __init__(self, x, y, proc_x=None, proc_y=None):\n",
    "        self.x =self.process(x, proc_x)\n",
    "        #set_trace()\n",
    "        self.y =self.process(y, proc_y)\n",
    "        self.proc_x = proc_x\n",
    "\n",
    "        self.proc_y = proc_x\n",
    "\n",
    "        self.proc_y = proc_y\n",
    "         \n",
    "    def __repr__(self): return f'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\n",
    "    def __getitem__(self,idx): return self.x[idx],self.y[idx]\n",
    "    def __len__(self): return len(self.x)\n",
    "        \n",
    "    @classmethod\n",
    "    def label_by_func(cls, il, f, proc_x=None, proc_y=None):\n",
    "        #set_trace()\n",
    "        return cls(il, _label_by_func(il, f), proc_x=proc_x, proc_y=proc_y)\n",
    "\n",
    "# make the training and validation set\n",
    "def label_by_func(sd, f, proc_x=None, proc_y=None):\n",
    "    train = LabelledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y)\n",
    "    valid = LabelledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y)\n",
    "    return SplitData(train,valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class - only an order\n",
    "class Transform():\n",
    "    _order = 0\n",
    "    \n",
    "class ResizeFixed(Transform):\n",
    "    _order=10\n",
    "    def __init__(self,size):\n",
    "        if isinstance(size,int): size=(size,size)\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR)\n",
    "\n",
    "# To byte tensor transform\n",
    "def toByteTensor(item):\n",
    "    res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))\n",
    "    w,h = item.size\n",
    "    return res.view(h,w,-1).permute(2,0,1)\n",
    "toByteTensor._order=20    \n",
    "\n",
    "# To float tensor\n",
    "def toFloatTensor(item): return item.float()\n",
    "toFloatTensor._order=30\n",
    "    \n",
    "# Grey to RBG\n",
    "class MakeRGB(Transform):\n",
    "    def __call__(self, item): return item.convert('RGB') # (item should be a PIL object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = Callback()\n",
    "tr = TrainEvalCallback()\n",
    "#x.__class__.__name__, x.name, tr.name\n",
    "\n",
    "\n",
    "def makeTestCB():\n",
    "    return TestCbsCallback()\n",
    "\n",
    "# Test exceptions\n",
    "class ExcpTest(Callback):\n",
    "    \n",
    "    def begin_epoch(self):\n",
    "        print(\"About to apply an exception\")\n",
    "        raise CancelEpochException()\n",
    "        \n",
    "    def after_cancel_train(self):\n",
    "        print(\"I told you I was cancelling the training wiht an exception\")\n",
    "\n",
    "#xCpt = ExcpTest()\n",
    "#run = LearnerRunnerDJ(1, 2, 3, cb_funcs= makeTestCB, cbs=xCpt)\n",
    "\n",
    "#run.fit(1)\n",
    "\n",
    "#run.cbs[0].run, run.cbs[0].name, run.cbs[1].name, run.train_eval\n",
    "\n",
    "\n",
    "#avgSts = AvgStats(accuracy, True)\n",
    "\n",
    "#x = tensor([1, 2])\n",
    "#y = tensor([[2, 6, 3], [2, 5, 3]])\n",
    "\n",
    "#accuracy(y, x)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test compose\n",
    "class add1():\n",
    "    def __init__(self):\n",
    "        self._order = 2\n",
    "        \n",
    "    def __call__(self, x): return x+1\n",
    "    \n",
    "class take3Div2():\n",
    "    def __init__(self):\n",
    "        self._order = 1\n",
    "    def __call__(self, x):return (x - 3)/2\n",
    "    \n",
    "\n",
    "xx = compose(1, [add1(), take3Div2()])\n",
    "#xx = listify([1])\n",
    "#ixinstance([1, 2], None)\n",
    "#type(None)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method test.print of <__main__.test object at 0x7f26c5e90320>>\n",
      "Hello\n",
      "0 a\n",
      "1 b\n",
      "2 c\n"
     ]
    }
   ],
   "source": [
    "class test():\n",
    "    \n",
    "    def print(self):\n",
    "        print(\"Hello\")\n",
    "        return True\n",
    "        \n",
    "    def __call__(self, func):\n",
    "        f = getattr(self, func, None)\n",
    "        print(f)\n",
    "        if f and f(): return True # note f() runs the function\n",
    "        return False\n",
    "    \n",
    "ttt = test()\n",
    "ttt('print')\n",
    "\n",
    "for i, j in enumerate([\"a\", \"b\", \"c\"]):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test list container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<function __main__.make_rgb(item)>], [<function __main__.make_rgb(item)>])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '/home/paul/fastaiMyData/bikes/'\n",
    "\n",
    "ii = ImageList.from_files(x )\n",
    "ii[5]\n",
    "# use the transforms to convert an image somehow\n",
    "class Transform(): _order=0\n",
    "\n",
    "class MakeRGB(Transform):\n",
    "    def __call__(self, item): return item.convert('LA')\n",
    "\n",
    "def make_rgb(item): return item.convert('LA')\n",
    "\n",
    "iii = ImageList.from_files(x, trnsfrms = [make_rgb])\n",
    "iii[6]\n",
    "imFn= iii.lst[8]\n",
    "imFn.parent.parent.name\n",
    "rand_splitter(imFn)\n",
    "\n",
    "iiii = iii.new(iii.lst[0:4])\n",
    "iii.trnsfrms, iiii.trnsfrms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'b', 'v', 'kkk'],\n",
       " [0, 1, 2, 1, 0, 3],\n",
       " {'a': 0, 'b': 1, 'v': 2, 'kkk': 3},\n",
       " ['a', 'b', 'v', 'kkk'],\n",
       " ['a', 'b', 'v', 'v', 'a', 'kkk'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test uniqueify\n",
    "s = [\"a\", \"b\", \"v\", \"b\", \"a\", \"kkk\"]\n",
    "sUni = uniqueify(s)\n",
    "# processor\n",
    "p = Processor()\n",
    "\n",
    "\n",
    "sUni, p(s), p.otoi, p.vocab, p.deProcess([0, 1, 2, 2, 0, 3])\n",
    "\n",
    "# Test processor on our  class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: (ImageList)(395 items)\n",
       "[PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000074.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000007.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000012.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000085.png'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000025.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000054.jpeg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000081.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000066.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000062.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000026.jpg')].../home/paul/fastaiMyData/bikes/ \n",
       "\n",
       "Valid: (ImageList)(183 items)\n",
       "[PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000094.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000029.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000093.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000086.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000051.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000098.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000065.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000070.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000044.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000053.jpg')].../home/paul/fastaiMyData/bikes/"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test splitter\n",
    "x = randSplitter(iii)\n",
    "len(x[0]), len(x[1])\n",
    "\n",
    "xxPro = Processor()\n",
    "xxPro([parent_labeler(x) for  x in iii.lst[3:5]])\n",
    "\n",
    "#xx = SplitData(x[0],x[1])\n",
    "xx = SplitData.split_by_func(iii, rand_splitter)\n",
    "\n",
    "# Test labelledata\n",
    "#lbDtConstructor = LabelledData\n",
    "#lbDtTrain = LabelledData.label_by_func(xx.train, parent_labeler)\n",
    "\n",
    "\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.Processor at 0x7f26c5de19b0>,\n",
       " <__main__.Processor at 0x7f26c5de19b0>,\n",
       " SplitData\n",
       " Train: LabelledData\n",
       " x: (ImageList)(395 items)\n",
       " [PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000074.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000007.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000012.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000085.png'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000025.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000054.jpeg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000081.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000066.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000062.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000026.jpg')].../home/paul/fastaiMyData/bikes/\n",
       " y: (ItemList)(395 items)\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0].../home/paul/fastaiMyData/bikes/\n",
       "  \n",
       " \n",
       " Valid: LabelledData\n",
       " x: (ImageList)(183 items)\n",
       " [PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000094.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000029.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000093.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000086.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000051.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000098.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000065.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000070.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000044.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000053.jpg')].../home/paul/fastaiMyData/bikes/\n",
       " y: (ItemList)(183 items)\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0].../home/paul/fastaiMyData/bikes/,\n",
       " LabelledData\n",
       " x: (ImageList)(183 items)\n",
       " [PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000094.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000029.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000093.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000086.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000051.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000098.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000065.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000070.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000044.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000053.jpg')].../home/paul/fastaiMyData/bikes/\n",
       " y: (ItemList)(183 items)\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0].../home/paul/fastaiMyData/bikes/)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test labelled data class (processor returns the integer labelling the class)\n",
    "lblDtTrain = LabelledData.label_by_func(iii, parent_labeler, proc_y = Processor())\n",
    "lblDtTrain.x, lblDtTrain.y\n",
    "#lblDtValid = LabelledData.label_by_func(xx.valid, parent_labeler, proc_y = Processor())\n",
    "\n",
    "\n",
    "allDat = label_by_func(xx, parent_labeler, proc_y=Processor())\n",
    "lblDtValid = LabelledData.label_by_func(xx.valid, parent_labeler, proc_y = allDat.train.proc_y)\n",
    "\n",
    "allDat.train.proc_y, allDat.valid.proc_y, allDat, lblDtValid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, <PIL.Image.Image image mode=LA size=500x375 at 0x7F26C5E90C50>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(toByteTensor(iii[0])), iii[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[192., 192., 192.,  ..., 195., 195., 195.],\n",
       "          [192., 192., 192.,  ..., 195., 195., 195.],\n",
       "          [192., 192., 192.,  ..., 195., 195., 195.],\n",
       "          ...,\n",
       "          [133., 134., 129.,  ..., 116., 113., 114.],\n",
       "          [133., 138., 140.,  ..., 133., 128., 124.],\n",
       "          [134., 133., 133.,  ..., 131., 130., 132.]],\n",
       " \n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]]),\n",
       " tensor([[[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]],\n",
       " \n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trnfms = [make_rgb, ResizeFixed(128), toByteTensor, toFloatTensor] # transforms to make the image rgb, then convert a batch\n",
    "# to byte tensors then to float tensors\n",
    "\n",
    "pth = '/home/paul/fastaiMyData/bikes/' # image path\n",
    "\n",
    "# use the from files decorator to get the images\n",
    "imLst = ImageList.from_files(pth, trnsfrms = trnfms)\n",
    "\n",
    "# Split data\n",
    "sd = SplitData.split_by_func(imLst, partial(rand_splitter, trainFrac = 0.7))\n",
    "\n",
    "# Label data\n",
    "lblDtTrn = LabelledData.label_by_func(sd.train, parent_labeler, proc_y=Processor())\n",
    "lblDtVal = LabelledData.label_by_func(sd.valid, parent_labeler, proc_y=lblDtTrn.proc_y)\n",
    "lblDt = SplitData(lblDtTrn, lblDtVal)\n",
    "\n",
    "lblDt.train.x[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]],\n",
       " \n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]]), 416, 162, LabelledData\n",
       " x: (ImageList)(416 items)\n",
       " [PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000074.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000007.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000012.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000085.png'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000025.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000054.jpeg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000094.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000029.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000080.jpg'), PosixPath('/home/paul/fastaiMyData/bikes/hybrids/00000028.jpg')].../home/paul/fastaiMyData/bikes/\n",
       " y: (ItemList)(416 items)\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0].../home/paul/fastaiMyData/bikes/)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imLst[8], len(lblDtTrn.y), len(lblDtVal.y), lblDt.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]],\n",
       "\n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
       "\n",
       "\n",
       "        [[[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]],\n",
       "\n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
       "\n",
       "\n",
       "        [[[239., 239., 239.,  ..., 239., 239., 239.],\n",
       "          [239., 239., 239.,  ..., 239., 239., 239.],\n",
       "          [239., 239., 239.,  ..., 239., 239., 239.],\n",
       "          ...,\n",
       "          [239., 239., 239.,  ..., 239., 239., 239.],\n",
       "          [239., 239., 239.,  ..., 239., 239., 239.],\n",
       "          [239., 239., 239.,  ..., 239., 239., 239.]],\n",
       "\n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[  0.,   7.,  15.,  ...,   0.,   0.,   0.],\n",
       "          [  0.,   7.,  15.,  ...,   0.,   0.,   0.],\n",
       "          [  0.,   7.,  15.,  ...,   0.,   0.,   0.],\n",
       "          ...,\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
       "\n",
       "\n",
       "        [[[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]],\n",
       "\n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
       "\n",
       "\n",
       "        [[[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]],\n",
       "\n",
       "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "          [255., 255., 255.,  ..., 255., 255., 255.]]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 32\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "train_dl = DataLoader(lblDt.train, bs, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(lblDtVal, bs, shuffle=False)\n",
    "\n",
    "\n",
    "class DataLoaderX():\n",
    "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs] # generator\n",
    "    \n",
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)    \n",
    "\n",
    "class DataLoaderXX():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])\n",
    "            \n",
    "testDl = iter(DataLoaderX([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 2))\n",
    "next(testDl)\n",
    "\n",
    "testDl2 = DataLoaderX(lblDt.train, 3)\n",
    "#for dx in enumerate(testDl2):\n",
    "    #print(dx)\n",
    "    \n",
    "\n",
    "    \n",
    "x, y = next(iter(train_dl))\n",
    "#x, y = next(iter(train_dl))\n",
    "#g = DataLoaderXX(lblDt.train, bs)\n",
    "#next(iter(g))\n",
    "\n",
    "#x = [lblDt.train[i] for i in range(4)]\n",
    "#type(lblDt.train)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databunch\n",
    "#export\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, c=None):\n",
    "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c # c is final number of acitvations\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "        \n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset\n",
    "    \n",
    "\n",
    "\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
