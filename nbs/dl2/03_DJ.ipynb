{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'datasets' from 'fastai' (/opt/conda/lib/python3.7/site-packages/fastai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e435fd089daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_02\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m \u001b[0;31m# all nn functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/notebooks/nbs/dl2/exp/nb_02.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# file to edit: dev_nb/02_fully_connected.ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_01\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/notebooks/nbs/dl2/exp/nb_01.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'datasets' from 'fastai' (/opt/conda/lib/python3.7/site-packages/fastai/__init__.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as F # all nn functions\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50\n",
    "n, m, nh\n",
    "type(n), type(x_train), n, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deifinition\n",
    "\n",
    "# model contains the layers and a forward function (__call__)\n",
    "class Model(nn.Module): # basically same as nn.module\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        super().__init__() # sets the set attribute to add the layers to the 'modules dictionary'\n",
    "        #self.layers = layers\n",
    "        #for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "        self.layers = nn.ModuleList(layers) # equivalent to the loop above ading modules in Pytorch\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "    \n",
    "#??nn.Module\n",
    "# Layers\n",
    "\n",
    "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]\n",
    "\n",
    "# New model object\n",
    "\n",
    "net1 = Model(layers)\n",
    "\n",
    "net2 = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)) # pytorch version\n",
    "\n",
    "net1, net2, net1.parameters(), # parameters generator function in nn.module generates the parameters \n",
    "#net1._modules\n",
    "\n",
    "#n = nn.Linear(m, nh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator example\n",
    "\n",
    "def genEx():\n",
    "    \n",
    "    x = [1, 2, 3, 4]\n",
    "    \n",
    "    for dx in x:\n",
    "        yield dx\n",
    "        \n",
    "ex = genEx()\n",
    "print(next(ex))\n",
    "print(next(ex))\n",
    "\n",
    "for dx in genEx():\n",
    "    \n",
    "    print(\"Genrating:\" +  str(dx))\n",
    "    \n",
    "ff = {\"a\" : 1, \"b\" : 2}\n",
    "ff.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 50\n",
    "#p = range((n-1)//bs + 1)\n",
    "#x = range((n - 1)//bs + 1)\n",
    "#xx = [i for i in x]\n",
    "\n",
    "type(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net1(x_train[0:50])\n",
    "loss = F.cross_entropy(pred, y_train[0:50])\n",
    "loss\n",
    "# Loss function \n",
    "#F.cross_entropy(net1())\n",
    "\n",
    "# Accurac\n",
    "\n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n",
    "#accuracyz = accuracy(f, trainDS.y[0:50])\n",
    "#torch.argmax(pred, dim = 1) == y_train[0:50]\n",
    "#pred.shape, F.shape, accuracyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic loop \n",
    "epochs = 1 # number of epochs\n",
    "bs = 64 # batch size\n",
    "lr = 0.5 # learning rate\n",
    "\n",
    "for dx in range(epochs):\n",
    "    for bdx in range((n-1)//bs + 1):\n",
    "        x = x_train[bdx*bs:bdx*bs + bs]\n",
    "        y = y_train[bdx*bs:bdx*bs + bs]\n",
    "        \n",
    "        #loss = loss_func(model(x), y)\n",
    "        loss = F.cross_entropy(net1(x), y) # forward to loss\n",
    "        \n",
    "        # backward loop\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for ldx in net1.layers: # loop through layers\n",
    "                if hasattr(ldx, 'weight'): # check for weight attributes\n",
    "                    ldx.weight -= lr*ldx.weight.grad # update weights\n",
    "                    ldx.bias -= lr*ldx.bias.grad # upate bias\n",
    "                    ldx.weight.grad.zero_()\n",
    "                    ldx.bias.grad.zero_()\n",
    "            \n",
    "#\n",
    "#print(\"Accuray after 1 epoch = \" + str(accuracy(F.cross_entropy(net1(x_train[0:50]), y_train[0:50]), y_train[0:50])))\n",
    "                    \n",
    "# Optimizer to refactor code (note only needed for loss(the backward loop, so only need to update layers \n",
    "# with parameters (i.e. not relu))loss\n",
    "\n",
    "class Optm():\n",
    "    \n",
    "    def __init__(self, params , lr = 0.5):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        \n",
    "    def step(self):\n",
    "        for ldx in self.params():\n",
    "            ldx -= lr*ldx.grad.zeros_()\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def zero_grad():\n",
    "        \n",
    "opt = Optm(net1.parameters())\n",
    "for dx in net1.parameters():\n",
    "    print(dx.shape)\n",
    "\n",
    "b = F.cross_entropy(net1(x_train[0:bs]), y_train[0:bs])\n",
    "b\n",
    "pred = net1(x_train[0:bs])\n",
    "pred.shape\n",
    "#a = accuracy(pred, y_train[0:bs])\n",
    "#accuracy = accuracy(pred, y_train[0:50])\n",
    "\n",
    "net1._modules, net1.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimiser- allows stepping through layers and updating and zeroing gradients \n",
    "# nn module parameters function returns tensor of layers with values\n",
    "## This is equivalent to optim.sdg from pytorch\n",
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params), lr\n",
    "        \n",
    "    def step(self): # step through each parameter updating the weights\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "\n",
    "    def zero_grad(self): # step through each parameter updating the gradients\n",
    "        for p in self.params: p.grad.data.zero_()\n",
    "            \n",
    "opt = Optimizer(net1.parameters())\n",
    "\n",
    "from torch import optim\n",
    "optTorch = optim.SGD(net1.parameters(), 0.5)\n",
    "\n",
    "opt.params, \n",
    "optTorch.param_groups\n",
    "\n",
    "\n",
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class genEx2():\n",
    "    def __init__(self):\n",
    "            self.x = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    def gen(self):\n",
    "        for ix in self.x:\n",
    "            yield ix\n",
    "\n",
    "\n",
    "g = genEx2()\n",
    "\n",
    "for gx in g.gen():\n",
    "    print(gx)\n",
    "    \n",
    "gg = genEx2()\n",
    "\n",
    "class genRec():\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        \n",
    "    def update(self):\n",
    "        for px in self.p:\n",
    "            px+= 1\n",
    "        \n",
    "gr = genRec(gg.gen())\n",
    "gr.update()\n",
    "\n",
    "for gx in gr.p:\n",
    "    print(gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it1 = iter([1, 2, 3])\n",
    "it2 = iter([5, 6, 7])\n",
    "\n",
    "next(it1), next(it2), next(it2), next(it1)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(next(it1))\n",
    "    except StopIteration:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our own classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - contains the x and y data. The __len__ method returns the length of the dataset \n",
    "# and the __getitem__ returns the x and y\n",
    "class Dataset():\n",
    "    def __init__(self,x, y):\n",
    "        self.x, self.y = x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "\n",
    "# Dataloader - uses a coroutine to request data from the dataset\n",
    "# Takes a dataset object and a batch size as input - this data loader allows sequenctial loading\n",
    "\n",
    "class Dataloader():\n",
    "    def __init__(self, ds, bs = 64):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]\n",
    "    def __len__(self): return len(self.ds)//self.bs\n",
    "            \n",
    "\n",
    "# Data loader and related classes for optional random sampling. It just gives random samples of values from the \n",
    "#dataset\n",
    "\n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "            \n",
    "# This class is different from Jeremys as is creates its own internal Sampler - \n",
    "#Jeremys allows the sampler to be passed in. This is probably more flexible\n",
    "class DataloaderShuffle():\n",
    "    def __init__(self, ds, bs, shuffle = False):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "        self.Smplr = (Sampler(ds, bs, shuffle))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for dx in self.Smplr:\n",
    "            #print(dx)\n",
    "            yield self.ds[dx] # this returns a tuple with index 0 containing the image and index 1 containing the\n",
    "            # training set. Jeremey uses a collate function to unpack the tuple.This is more flexible can add padingeetc\n",
    "    def __len__(self): return len(self.ds)//self.bs\n",
    "# Define the model class =s=================================================================================== #\n",
    "# pytorch type model, inheriting from nn.module The __call__ function is the forward pass\n",
    "# nn layers can be passed to the model and registered using the nn.ModuleList function\n",
    "# \n",
    "class nnModel(nn.Module):  \n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "## Define the optimizer class ================================================================================ #\n",
    "class Optimizer():\n",
    "    def __init__(self, params, lr = 0.5):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        \n",
    "    def step(self): # steps through the parameters on the backward pass and updates them\n",
    "        with torch.no_grad():\n",
    "            for dx in self.params:\n",
    "                dx -= dx.grad*lr\n",
    "        \n",
    "    def zero_grad(self): # zeros the gradients\n",
    "        for p in self.params: p.grad.data.zero_()\n",
    "\n",
    "## Define a minimum cross entropy loss function\n",
    "#class MinCrossEntropy loss:\n",
    "\n",
    "# Training loop \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Handle batchnorm / dropout\n",
    "        model.train()\n",
    "#         print(model.training)\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            #print(\"In trainign\" + str(epx))\n",
    "        print(\"Training ac, epoch: \" + str( epoch ) + \" loss: \" +str(loss))\n",
    "        model.eval()\n",
    "#         print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv, 180)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequential data loader\n",
    "ds_train = Dataset(x_train[0:29], y_train[0:29])\n",
    "\n",
    "ld = Dataloader(ds_train, 3)\n",
    "ldIt = iter(ld)\n",
    "sx, sy = next((ldIt))\n",
    "sx2, sy2 = next((ldIt))\n",
    "sx3,sy3 = next(ldIt)\n",
    "assert sy.shape == (3,)\n",
    "sy, sy2, sy3 \n",
    "\n",
    "\n",
    "# Sampler test ================================================================================================== #\n",
    "smp = iter(Sampler(ds_train, 3, True))\n",
    "smp1 = next(smp)\n",
    "smp2 = next(smp)\n",
    "smp1, smp2\n",
    "\n",
    "smpF = iter(Sampler(ds_train, 3, True))\n",
    "#for dx in smpF:\n",
    " #   print(dx)\n",
    "#smp1F = next(smpF)\n",
    "#smp2F = next(smpF)\n",
    "#smp1F, smp2F\n",
    "\n",
    "# Data loader with shuffler\n",
    "\n",
    "#ld2 = DataloaderShuffle(ds_train, 3, True)\n",
    "#ld2It = iter(ld2)\n",
    "#ds, dt= next(ld2It)\n",
    "#sy, y\n",
    "#plt.imshow(sxx[0].view(28,28))\n",
    "#syy[0]\n",
    "\n",
    "#for ex in ld2It:\n",
    "#    print(ex)\n",
    "\n",
    "epochs = 5\n",
    "bs = 50\n",
    "\n",
    "## Set nn model\n",
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10)]\n",
    "model1 = nnModel(layers)\n",
    "    \n",
    "## Set optizer class\n",
    "optim = Optimizer(model1.parameters())\n",
    "\n",
    "## Set dataset and data loader\n",
    "trainDS = Dataset(x_train, y_train)\n",
    "trainDLdr = DataloaderShuffle(trainDS, bs, True)\n",
    "\n",
    "validDS = Dataset(x_valid, y_valid)\n",
    "validDldr = DataloaderShuffle(validDS, bs, True)\n",
    "\n",
    "ac1 = accuracy(model1(trainDS.x[0:1000]), trainDS.y[0:1000])\n",
    "ac0 = accuracy(model1(validDS.x[0:1000]), trainDS.y[0:1000])\n",
    "## Training loop \n",
    "\n",
    "#for epx in range(epochs):\n",
    " #   model1.train()\n",
    "  #  for x,y in trainDLdr:\n",
    "        #print(x.shape,y)\n",
    "   #     pred = model1(x)\n",
    "    #    loss = F.cross_entropy(pred, y)\n",
    "     #   loss.backward()\n",
    "      #  optim.step()\n",
    "       # optim.zeroGrad()\n",
    "    \n",
    "    ## check accuracy on validation set\n",
    "  #  model1.eval()\n",
    "   # with torch.no_grad():\n",
    "    #    tot_loss,tot_acc = 0.,0.\n",
    "     #   print(epx)\n",
    "      #  for xb,yb in validDldr:\n",
    "#             print(epx)\n",
    "#             pred = model1(xb)\n",
    "#             tot_loss += F.cross_entropy(pred, yb)\n",
    "#             tot_acc  += accuracy(pred,yb)\n",
    "#             #print(F.cross_entropy(pred, yb))\n",
    "#             #print(\"total loss =\", str(tot_loss))\n",
    "#     nv = len(validDS)\n",
    "#     print(epx, tot_loss/nv, tot_acc/nv)\n",
    "    #return tot_loss/nv, tot_acc/nv\n",
    "fit(epochs, model1, F.cross_entropy, optim, trainDLdr, validDldr)\n",
    "        \n",
    "#, ds_train.y, y_train[0:28], plt.imshow(ds_train.x[3].view(28,28))\n",
    "ac2 = accuracy(model1(trainDS.x[0:1000]), trainDS.y[0:1000])\n",
    "ac3 = accuracy(model1(validDS.x[0:1000]), validDS.y[0:1000])\n",
    "#ac1, ac0, ac2, ac3\n",
    "\n",
    "x = model1(validDS.x[0:10])\n",
    "y = validDS.y[0:10]\n",
    "acc = accuracy(x,y)\n",
    "\n",
    "ac1, ac0, ac2, ac3\n",
    "#len(validDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rndprm():\n",
    "    def __init__(self):\n",
    "        self.x = torch.randperm(5)\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for dx in range(5):\n",
    "            yield self.x[dx]\n",
    "            \n",
    "x = iter(rndprm())\n",
    "for dx in x:\n",
    "    print(dx)\n",
    "    \n",
    "print(\"Going again\")\n",
    "\n",
    "for dx in x:\n",
    "    print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.randperm(20)\n",
    "d[0:2]\n",
    "x = (1, 2)\n",
    "a, b = zip(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
