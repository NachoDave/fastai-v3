{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as F # all nn functions\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, torch.Tensor, 50000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50\n",
    "n, m, nh\n",
    "type(n), type(x_train), n, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "   )\n",
       " ), Sequential(\n",
       "   (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=50, out_features=10, bias=True)\n",
       " ), <generator object Module.parameters at 0x7f233c7b0390>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deifinition\n",
    "\n",
    "# model contains the layers and a forward function (__call__)\n",
    "class Model(nn.Module): # basically same as nn.module\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        super().__init__() # sets the set attribute to add the layers to the 'modules dictionary'\n",
    "        #self.layers = layers\n",
    "        #for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "        self.layers = nn.ModuleList(layers) # equivalent to the loop above ading modules in Pytorch\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "    \n",
    "#??nn.Module\n",
    "# Layers\n",
    "\n",
    "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]\n",
    "\n",
    "# New model object\n",
    "\n",
    "net1 = Model(layers)\n",
    "\n",
    "net2 = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)) # pytorch version\n",
    "\n",
    "net1, net2, net1.parameters(), # parameters generator function in nn.module generates the parameters \n",
    "#net1._modules\n",
    "\n",
    "#n = nn.Linear(m, nh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Genrating:1\n",
      "Genrating:2\n",
      "Genrating:3\n",
      "Genrating:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_values([1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator example\n",
    "\n",
    "def genEx():\n",
    "    \n",
    "    x = [1, 2, 3, 4]\n",
    "    \n",
    "    for dx in x:\n",
    "        yield dx\n",
    "        \n",
    "ex = genEx()\n",
    "print(next(ex))\n",
    "print(next(ex))\n",
    "\n",
    "for dx in genEx():\n",
    "    \n",
    "    print(\"Genrating:\" +  str(dx))\n",
    "    \n",
    "ff = {\"a\" : 1, \"b\" : 2}\n",
    "ff.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 50\n",
    "#p = range((n-1)//bs + 1)\n",
    "#x = range((n - 1)//bs + 1)\n",
    "#xx = [i for i in x]\n",
    "\n",
    "type(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net1(x_train[0:50])\n",
    "loss = F.cross_entropy(pred, y_train[0:50])\n",
    "loss\n",
    "# Loss function \n",
    "#F.cross_entropy(net1())\n",
    "\n",
    "# Accurac\n",
    "\n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n",
    "#accuracyz = accuracy(f, trainDS.y[0:50])\n",
    "#torch.argmax(pred, dim = 1) == y_train[0:50]\n",
    "#pred.shape, F.shape, accuracyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('layers', ModuleList(\n",
       "                 (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "                 (1): ReLU()\n",
       "                 (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "               ))]), <generator object Module.parameters at 0x7f233c7b0a98>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic loop \n",
    "epochs = 1 # number of epochs\n",
    "bs = 64 # batch size\n",
    "lr = 0.5 # learning rate\n",
    "\n",
    "for dx in range(epochs):\n",
    "    for bdx in range((n-1)//bs + 1):\n",
    "        x = x_train[bdx*bs:bdx*bs + bs]\n",
    "        y = y_train[bdx*bs:bdx*bs + bs]\n",
    "        \n",
    "        #loss = loss_func(model(x), y)\n",
    "        loss = F.cross_entropy(net1(x), y) # forward to loss\n",
    "        \n",
    "        # backward loop\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for ldx in net1.layers: # loop through layers\n",
    "                if hasattr(ldx, 'weight'): # check for weight attributes\n",
    "                    ldx.weight -= lr*ldx.weight.grad # update weights\n",
    "                    ldx.bias -= lr*ldx.bias.grad # upate bias\n",
    "                    ldx.weight.grad.zero_()\n",
    "                    ldx.bias.grad.zero_()\n",
    "            \n",
    "#\n",
    "#print(\"Accuray after 1 epoch = \" + str(accuracy(F.cross_entropy(net1(x_train[0:50]), y_train[0:50]), y_train[0:50])))\n",
    "                    \n",
    "# Optimizer to refactor code (note only needed for loss(the backward loop, so only need to update layers \n",
    "# with parameters (i.e. not relu))loss\n",
    "\n",
    "class Optm():\n",
    "    \n",
    "    def __init__(self, params , lr = 0.5):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        \n",
    "    def step(self):\n",
    "        for ldx in self.params():\n",
    "            ldx -= lr*ldx.grad.zeros_()\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def zero_grad():\n",
    "        \n",
    "opt = Optm(net1.parameters())\n",
    "for dx in net1.parameters():\n",
    "    print(dx.shape)\n",
    "\n",
    "b = F.cross_entropy(net1(x_train[0:bs]), y_train[0:bs])\n",
    "b\n",
    "pred = net1(x_train[0:bs])\n",
    "pred.shape\n",
    "#a = accuracy(pred, y_train[0:bs])\n",
    "#accuracy = accuracy(pred, y_train[0:50])\n",
    "\n",
    "net1._modules, net1.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[-0.0157, -0.0205, -0.0112,  ...,  0.0241, -0.0124,  0.0339],\n",
       "           [ 0.0218,  0.0145,  0.0355,  ...,  0.0337,  0.0188,  0.0117],\n",
       "           [-0.0227, -0.0309,  0.0171,  ..., -0.0287,  0.0142,  0.0295],\n",
       "           ...,\n",
       "           [ 0.0120,  0.0279,  0.0191,  ..., -0.0321,  0.0180,  0.0186],\n",
       "           [-0.0270,  0.0015, -0.0139,  ...,  0.0297, -0.0299, -0.0031],\n",
       "           [ 0.0076,  0.0303, -0.0285,  ...,  0.0089, -0.0148,  0.0097]],\n",
       "          requires_grad=True), Parameter containing:\n",
       "   tensor([-5.9563e-02, -2.7065e-04,  1.1473e-01,  9.5647e-03,  1.9103e-01,\n",
       "           -1.0728e-01,  5.8867e-02,  1.2190e-01, -1.3886e-01,  2.5983e-02,\n",
       "            5.6243e-02, -1.4118e-01,  2.1474e-01,  1.6607e-03,  4.1303e-02,\n",
       "            7.0366e-02,  1.1198e-01,  4.4891e-02,  6.0844e-02,  5.3385e-02,\n",
       "            9.6807e-02,  8.8706e-02,  1.7028e-01,  1.0205e-01,  7.1995e-02,\n",
       "            1.5223e-01, -1.7794e-01,  4.8776e-02,  1.2203e-03,  5.2614e-01,\n",
       "           -1.5594e-02, -3.1546e-01, -3.1086e-01,  7.8993e-02, -2.7978e-02,\n",
       "            3.9629e-02,  7.2247e-02, -2.1371e-01,  2.7498e-01, -1.2786e-01,\n",
       "            9.2153e-02,  6.2166e-02,  2.1837e-01,  1.6490e-01,  5.9393e-02,\n",
       "           -2.8550e-01, -1.5313e-03, -1.7475e-01,  1.7689e-01,  1.9256e-01],\n",
       "          requires_grad=True), Parameter containing:\n",
       "   tensor([[-5.1360e-01, -9.8101e-02, -1.3332e-01,  4.9256e-01, -6.3866e-02,\n",
       "            -1.3471e-01, -6.1762e-01, -3.6500e-01,  3.4493e-01, -1.3358e-01,\n",
       "            -5.2891e-03, -1.9277e-01,  9.5569e-02, -2.0769e-03, -1.8318e-01,\n",
       "            -2.3138e-01,  3.6132e-02,  8.5676e-02, -2.5378e-01,  8.3807e-02,\n",
       "            -7.6353e-02,  4.8369e-02,  1.2351e-01, -5.1010e-01,  2.0604e-01,\n",
       "            -5.9090e-01,  8.8826e-02, -2.8920e-01,  1.7495e-01,  6.3760e-01,\n",
       "             3.4707e-01,  2.9972e-01, -1.6300e-01,  5.8658e-01,  6.9264e-02,\n",
       "            -7.3802e-02, -7.3447e-02,  5.1508e-01,  1.3256e-01,  6.5398e-01,\n",
       "            -3.9021e-01, -3.5035e-01, -2.1157e-01,  5.4820e-01, -2.2024e-01,\n",
       "            -1.4380e-01, -2.8519e-01,  8.4533e-02, -5.1106e-01,  3.0652e-01],\n",
       "           [ 2.6395e-01, -3.2572e-02,  5.7515e-01,  3.0606e-01, -4.2284e-01,\n",
       "             2.4709e-02,  3.9085e-01, -1.7418e-01, -1.6787e-01,  6.2544e-01,\n",
       "             2.8582e-01,  4.5020e-01,  1.5067e-01, -8.5316e-02,  3.9613e-02,\n",
       "            -3.7764e-01,  9.1277e-01,  4.0033e-02, -3.1836e-01, -2.2460e-01,\n",
       "            -4.5023e-01,  2.1845e-01, -6.7330e-01,  1.8622e-01, -4.9976e-01,\n",
       "             5.9657e-01, -3.4610e-01, -6.3184e-02,  1.5629e-01,  2.0852e-01,\n",
       "            -4.6491e-01, -2.1968e-01, -5.0010e-01, -3.0655e-01,  3.0685e-01,\n",
       "            -4.3912e-02,  3.9306e-01, -1.0536e-01,  2.9168e-01, -1.0429e-01,\n",
       "            -5.4536e-01, -1.3359e-01,  4.3790e-01, -4.4154e-01,  4.0507e-01,\n",
       "             1.7288e-01, -1.3510e-03,  1.5183e-01, -3.0127e-01, -1.3334e-01],\n",
       "           [ 2.1107e-01,  2.1510e-03, -2.6603e-01, -5.7584e-01, -2.8754e-01,\n",
       "            -4.8341e-01, -5.3244e-03, -2.8519e-01,  1.4746e-01,  5.3308e-01,\n",
       "             3.9647e-01,  4.9075e-01,  3.8572e-02, -5.1159e-02, -8.4384e-02,\n",
       "             8.4948e-01, -1.9580e-01,  2.2117e-01,  4.7471e-01, -4.5692e-01,\n",
       "             2.2532e-01, -6.0307e-02, -1.9323e-01,  3.4183e-01, -3.5566e-01,\n",
       "            -1.8421e-01, -2.2517e-02,  2.8363e-01,  2.3916e-01,  1.2849e-01,\n",
       "            -2.3053e-02,  3.1512e-01, -1.5247e-01,  6.6906e-01,  6.3513e-01,\n",
       "             4.8023e-01, -3.9534e-01,  2.5019e-01, -7.7871e-01, -1.7115e-01,\n",
       "             1.7872e-01,  3.8997e-01,  8.4784e-03,  9.5191e-02,  4.3396e-04,\n",
       "             5.1074e-01,  2.7837e-01, -3.4512e-01, -2.9230e-01, -2.3537e-01],\n",
       "           [ 7.3322e-01, -1.4440e-01, -2.5270e-01, -3.4214e-01, -6.0369e-02,\n",
       "             3.5829e-01,  6.5385e-01,  6.8858e-01, -2.2525e-01, -2.1072e-01,\n",
       "             2.9009e-01, -3.0520e-01, -2.8287e-01, -7.5470e-02, -1.7063e-02,\n",
       "            -2.3101e-01, -2.0949e-01, -1.3783e-01, -2.7044e-01, -2.6130e-01,\n",
       "             1.7366e-01, -2.1137e-01,  1.5906e-01, -2.2205e-01, -5.2261e-01,\n",
       "             1.5597e-02,  1.9045e-01,  2.2840e-01, -1.3118e-01,  8.2843e-05,\n",
       "            -2.7354e-01,  3.2619e-01, -6.0012e-02, -3.6976e-02,  5.5194e-01,\n",
       "            -3.5010e-02, -1.1024e-01, -5.0591e-01, -1.8210e-01,  2.4008e-01,\n",
       "            -1.1470e-01, -1.0253e-01, -3.7307e-01, -2.9194e-01, -3.2479e-01,\n",
       "             7.8643e-01, -1.9918e-01, -2.5043e-01, -1.0776e-01,  5.5829e-01],\n",
       "           [-2.1522e-01, -1.8081e-01,  1.9038e-02,  1.8302e-01,  1.3690e-01,\n",
       "             1.3115e-01, -3.3565e-01,  4.1486e-01,  4.8629e-01, -1.1881e-01,\n",
       "            -5.8419e-01, -5.8680e-02, -2.1333e-01,  7.2439e-02, -8.5031e-03,\n",
       "            -2.0788e-01,  3.4339e-02,  8.0219e-02,  8.5335e-01,  5.7616e-02,\n",
       "             3.9567e-02,  6.0073e-01, -4.8078e-01,  5.4230e-01, -1.8303e-01,\n",
       "            -1.7095e-01, -2.8027e-01, -4.1070e-02, -3.3797e-01, -5.7482e-01,\n",
       "             3.4921e-01, -4.7182e-01,  1.0103e-01, -4.0302e-01, -1.0143e-01,\n",
       "             5.2281e-01, -5.6598e-02, -2.5454e-01, -3.0270e-01, -7.4994e-03,\n",
       "            -1.4901e-01,  2.5149e-01, -3.2085e-02,  2.4269e-01,  1.4805e-01,\n",
       "            -9.3968e-01,  1.1007e-02,  5.5565e-01,  5.4018e-01, -4.9604e-01],\n",
       "           [ 4.4139e-02,  2.3585e-01,  3.6223e-01, -5.8871e-02,  1.4195e-01,\n",
       "             1.9528e-01,  6.7794e-01,  9.8656e-02, -7.2953e-01, -5.2160e-02,\n",
       "            -4.7160e-01, -6.2861e-01, -2.0753e-01, -6.2397e-02, -2.4974e-02,\n",
       "            -1.0487e-01,  8.9824e-02,  1.7244e-01, -1.0253e-01,  3.4838e-01,\n",
       "             8.8625e-02, -4.6892e-01,  5.4311e-01, -2.3855e-01,  7.3773e-01,\n",
       "             5.7096e-01,  2.0456e-01,  1.6229e-01,  5.8177e-02,  5.3263e-01,\n",
       "             2.9786e-01, -3.3713e-01,  2.2259e-01,  8.7868e-03, -3.3938e-01,\n",
       "            -4.5426e-02,  4.7573e-01, -4.0501e-01,  9.3099e-01, -3.4437e-01,\n",
       "            -3.7688e-01, -4.7078e-01,  4.9696e-01,  6.3193e-03, -4.1944e-01,\n",
       "            -5.2953e-01,  1.8126e-01, -7.8332e-01,  5.8821e-01,  2.2420e-01],\n",
       "           [-4.1328e-01, -6.3130e-02,  4.9765e-02,  1.6086e-01, -6.2623e-01,\n",
       "            -9.0234e-02, -7.2911e-01, -4.2029e-01,  6.2585e-02,  4.6263e-02,\n",
       "            -2.0563e-01, -2.8067e-01,  1.3053e-01, -1.0394e-01,  7.8298e-02,\n",
       "             3.9030e-01,  3.7998e-02, -5.2041e-02,  5.4558e-02,  7.8813e-01,\n",
       "             6.8250e-01,  2.6703e-01, -5.0282e-01,  2.3468e-01,  5.2549e-01,\n",
       "             6.0870e-01,  2.9225e-02,  1.1383e-01,  6.0260e-01, -7.6487e-02,\n",
       "             3.5041e-01, -4.3711e-01, -3.8061e-01,  4.5262e-02, -3.1148e-01,\n",
       "             8.9065e-02,  4.0161e-01,  9.2717e-02, -4.0735e-02,  1.4370e-01,\n",
       "             5.0312e-02, -1.2654e-01, -4.9723e-01,  1.0703e-01,  4.2088e-01,\n",
       "            -6.7302e-01, -1.9477e-01, -2.2519e-01, -4.3370e-01, -7.0122e-03],\n",
       "           [-2.5312e-01,  7.1257e-02,  2.3754e-01,  1.2955e-01,  5.5628e-01,\n",
       "            -5.4649e-01, -2.2390e-01,  2.0533e-01, -2.9234e-01,  3.6830e-01,\n",
       "             1.8676e-01,  2.3966e-01,  8.5653e-01, -4.1614e-02,  2.8028e-01,\n",
       "            -5.3835e-02,  1.5359e-01,  1.8693e-01,  7.2761e-02,  3.3994e-01,\n",
       "             2.6203e-01, -1.0742e-01,  2.7580e-01, -5.2597e-01, -2.3770e-01,\n",
       "            -2.6164e-01, -1.6447e-01, -2.8470e-01, -3.3947e-01,  3.1065e-01,\n",
       "            -2.1093e-01, -4.1773e-02, -3.7203e-01, -3.5806e-01,  7.7468e-02,\n",
       "            -2.6462e-01, -1.9863e-01, -1.5537e-01, -1.4269e-01, -2.9585e-02,\n",
       "             8.5502e-01,  4.1795e-01,  5.3772e-01, -5.4549e-03, -1.4452e-01,\n",
       "             8.0177e-01, -2.2682e-02,  2.9449e-01, -2.2982e-01,  4.7384e-01],\n",
       "           [ 2.9390e-01, -7.3607e-02, -4.2673e-01, -3.2830e-01, -3.0592e-01,\n",
       "             5.8793e-01, -3.7870e-01, -6.3175e-01, -1.3563e-01, -7.0004e-01,\n",
       "             4.7880e-02,  3.0186e-01, -2.1643e-01,  1.3366e-01,  5.8630e-02,\n",
       "            -3.9618e-01, -3.9868e-01, -7.0525e-03, -3.9553e-01, -8.4396e-02,\n",
       "            -3.0840e-01, -7.6718e-01,  2.3227e-01, -2.7744e-01,  3.1707e-01,\n",
       "            -1.0275e-01,  2.3277e-01,  4.1665e-01,  3.9478e-01, -6.6251e-01,\n",
       "             3.5383e-01,  4.1387e-01,  4.8188e-01, -3.6050e-01, -2.6284e-01,\n",
       "            -2.8361e-01, -1.8475e-02,  5.6356e-01, -2.7373e-01, -2.4317e-01,\n",
       "            -2.2832e-01, -8.9246e-03, -1.4603e-01, -3.0998e-02, -6.1466e-03,\n",
       "             1.7297e-01,  2.3992e-01,  3.8067e-01, -2.3972e-02, -3.1985e-01],\n",
       "           [-2.9616e-01, -1.0500e-01,  1.8421e-01, -6.3097e-02,  6.9305e-01,\n",
       "            -2.3562e-03,  3.5773e-01,  6.1560e-01,  5.1560e-01, -2.9058e-01,\n",
       "             1.9005e-01, -1.4466e-01, -5.0535e-01,  7.1718e-02, -1.5473e-01,\n",
       "            -2.0691e-01, -2.1334e-01, -1.5481e-01, -3.7345e-01, -5.1916e-01,\n",
       "            -8.0073e-01,  4.5445e-01,  1.9592e-01,  4.3099e-01,  1.4123e-01,\n",
       "            -3.7528e-01,  1.8973e-01, -6.9389e-01, -6.1248e-01, -4.1127e-01,\n",
       "            -2.9700e-01,  8.0868e-02,  2.5170e-01, -1.9320e-01, -1.4254e-01,\n",
       "            -8.7337e-02,  1.3347e-01,  1.7938e-01,  1.7700e-01, -2.7971e-02,\n",
       "             4.6511e-01, -2.3379e-01, -1.5493e-01, -5.8010e-01,  2.6084e-01,\n",
       "            -4.1170e-02, -3.1675e-01,  6.3591e-01,  3.1483e-01, -3.1032e-01]],\n",
       "          requires_grad=True), Parameter containing:\n",
       "   tensor([-0.3508, -0.2241, -0.2250, -0.0850,  0.0590,  0.5423, -0.3346,  0.2885,\n",
       "            0.2568, -0.0532], requires_grad=True)],\n",
       "  'lr': 0.5,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Optimiser- allows stepping through layers and updating and zeroing gradients \n",
    "# nn module parameters function returns tensor of layers with values\n",
    "## This is equivalent to optim.sdg from pytorch\n",
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params), lr\n",
    "        \n",
    "    def step(self): # step through each parameter updating the weights\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "\n",
    "    def zero_grad(self): # step through each parameter updating the gradients\n",
    "        for p in self.params: p.grad.data.zero_()\n",
    "            \n",
    "opt = Optimizer(net1.parameters())\n",
    "\n",
    "from torch import optim\n",
    "optTorch = optim.SGD(net1.parameters(), 0.5)\n",
    "\n",
    "opt.params, \n",
    "optTorch.param_groups\n",
    "\n",
    "\n",
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "class genEx2():\n",
    "    def __init__(self):\n",
    "            self.x = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    def gen(self):\n",
    "        for ix in self.x:\n",
    "            yield ix\n",
    "\n",
    "\n",
    "g = genEx2()\n",
    "\n",
    "for gx in g.gen():\n",
    "    print(gx)\n",
    "    \n",
    "gg = genEx2()\n",
    "\n",
    "class genRec():\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        \n",
    "    def update(self):\n",
    "        for px in self.p:\n",
    "            px+= 1\n",
    "        \n",
    "gr = genRec(gg.gen())\n",
    "gr.update()\n",
    "\n",
    "for gx in gr.p:\n",
    "    print(gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "it1 = iter([1, 2, 3])\n",
    "it2 = iter([5, 6, 7])\n",
    "\n",
    "next(it1), next(it2), next(it2), next(it1)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(next(it1))\n",
    "    except StopIteration:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our own classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - contains the x and y data. The __len__ method returns the length of the dataset \n",
    "# and the __getitem__ returns the x and y\n",
    "class Dataset():\n",
    "    def __init__(self,x, y):\n",
    "        self.x, self.y = x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "\n",
    "# Dataloader - uses a coroutine to request data from the dataset\n",
    "# Takes a dataset object and a batch size as input - this data loader allows sequenctial loading\n",
    "\n",
    "class Dataloader():\n",
    "    def __init__(self, ds, bs = 64):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]\n",
    "    def __len__(self): return len(self.ds)//self.bs\n",
    "            \n",
    "\n",
    "# Data loader and related classes for optional random sampling. It just gives random samples of values from the \n",
    "#dataset\n",
    "\n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "            \n",
    "# This class is different from Jeremys as is creates its own internal Sampler - \n",
    "#Jeremys allows the sampler to be passed in. This is probably more flexible\n",
    "class DataloaderShuffle():\n",
    "    def __init__(self, ds, bs, shuffle = False):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "        self.Smplr = (Sampler(ds, bs, shuffle))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for dx in self.Smplr:\n",
    "            #print(dx)\n",
    "            yield self.ds[dx] # this returns a tuple with index 0 containing the image and index 1 containing the\n",
    "            # training set. Jeremey uses a collate function to unpack the tuple.This is more flexible can add padingeetc\n",
    "    def __len__(self): return len(self.ds)//self.bs\n",
    "# Define the model class =s=================================================================================== #\n",
    "# pytorch type model, inheriting from nn.module The __call__ function is the forward pass\n",
    "# nn layers can be passed to the model and registered using the nn.ModuleList function\n",
    "# \n",
    "class nnModel(nn.Module):  \n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "## Define the optimizer class ================================================================================ #\n",
    "class Optimizer():\n",
    "    def __init__(self, params, lr = 0.5):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        \n",
    "    def step(self): # steps through the parameters on the backward pass and updates them\n",
    "        with torch.no_grad():\n",
    "            for dx in self.params:\n",
    "                dx -= dx.grad*lr\n",
    "        \n",
    "    def zero_grad(self): # zeros the gradients\n",
    "        for p in self.params: p.grad.data.zero_()\n",
    "\n",
    "## Define a minimum cross entropy loss function\n",
    "#class MinCrossEntropy loss:\n",
    "\n",
    "# Training loop \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Handle batchnorm / dropout\n",
    "        model.train()\n",
    "#         print(model.training)\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            #print(\"In trainign\" + str(epx))\n",
    "        print(\"Training ac, epoch: \" + str( epoch ) + \" loss: \" +str(loss))\n",
    "        model.eval()\n",
    "#         print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv, 180)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ac, epoch: 0 loss: tensor(0.0998, grad_fn=<NllLossBackward>)\n",
      "0 tensor(0.1466) tensor(0.9572) 180\n",
      "Training ac, epoch: 1 loss: tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "1 tensor(0.1174) tensor(0.9649) 180\n",
      "Training ac, epoch: 2 loss: tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "2 tensor(0.1066) tensor(0.9680) 180\n",
      "Training ac, epoch: 3 loss: tensor(0.1148, grad_fn=<NllLossBackward>)\n",
      "3 tensor(0.1100) tensor(0.9675) 180\n",
      "Training ac, epoch: 4 loss: tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "4 tensor(0.0968) tensor(0.9740) 180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0690), tensor(0.1160), tensor(0.9830), tensor(0.9680))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sequential data loader\n",
    "ds_train = Dataset(x_train[0:29], y_train[0:29])\n",
    "\n",
    "ld = Dataloader(ds_train, 3)\n",
    "ldIt = iter(ld)\n",
    "sx, sy = next((ldIt))\n",
    "sx2, sy2 = next((ldIt))\n",
    "sx3,sy3 = next(ldIt)\n",
    "assert sy.shape == (3,)\n",
    "sy, sy2, sy3 \n",
    "\n",
    "\n",
    "# Sampler test ================================================================================================== #\n",
    "smp = iter(Sampler(ds_train, 3, True))\n",
    "smp1 = next(smp)\n",
    "smp2 = next(smp)\n",
    "smp1, smp2\n",
    "\n",
    "smpF = iter(Sampler(ds_train, 3, True))\n",
    "#for dx in smpF:\n",
    " #   print(dx)\n",
    "#smp1F = next(smpF)\n",
    "#smp2F = next(smpF)\n",
    "#smp1F, smp2F\n",
    "\n",
    "# Data loader with shuffler\n",
    "\n",
    "#ld2 = DataloaderShuffle(ds_train, 3, True)\n",
    "#ld2It = iter(ld2)\n",
    "#ds, dt= next(ld2It)\n",
    "#sy, y\n",
    "#plt.imshow(sxx[0].view(28,28))\n",
    "#syy[0]\n",
    "\n",
    "#for ex in ld2It:\n",
    "#    print(ex)\n",
    "\n",
    "epochs = 5\n",
    "bs = 50\n",
    "\n",
    "## Set nn model\n",
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10)]\n",
    "model1 = nnModel(layers)\n",
    "    \n",
    "## Set optizer class\n",
    "optim = Optimizer(model1.parameters())\n",
    "\n",
    "## Set dataset and data loader\n",
    "trainDS = Dataset(x_train, y_train)\n",
    "trainDLdr = DataloaderShuffle(trainDS, bs, True)\n",
    "\n",
    "validDS = Dataset(x_valid, y_valid)\n",
    "validDldr = DataloaderShuffle(validDS, bs, True)\n",
    "\n",
    "ac1 = accuracy(model1(trainDS.x[0:1000]), trainDS.y[0:1000])\n",
    "ac0 = accuracy(model1(validDS.x[0:1000]), trainDS.y[0:1000])\n",
    "## Training loop \n",
    "\n",
    "#for epx in range(epochs):\n",
    " #   model1.train()\n",
    "  #  for x,y in trainDLdr:\n",
    "        #print(x.shape,y)\n",
    "   #     pred = model1(x)\n",
    "    #    loss = F.cross_entropy(pred, y)\n",
    "     #   loss.backward()\n",
    "      #  optim.step()\n",
    "       # optim.zeroGrad()\n",
    "    \n",
    "    ## check accuracy on validation set\n",
    "  #  model1.eval()\n",
    "   # with torch.no_grad():\n",
    "    #    tot_loss,tot_acc = 0.,0.\n",
    "     #   print(epx)\n",
    "      #  for xb,yb in validDldr:\n",
    "#             print(epx)\n",
    "#             pred = model1(xb)\n",
    "#             tot_loss += F.cross_entropy(pred, yb)\n",
    "#             tot_acc  += accuracy(pred,yb)\n",
    "#             #print(F.cross_entropy(pred, yb))\n",
    "#             #print(\"total loss =\", str(tot_loss))\n",
    "#     nv = len(validDS)\n",
    "#     print(epx, tot_loss/nv, tot_acc/nv)\n",
    "    #return tot_loss/nv, tot_acc/nv\n",
    "fit(epochs, model1, F.cross_entropy, optim, trainDLdr, validDldr)\n",
    "        \n",
    "#, ds_train.y, y_train[0:28], plt.imshow(ds_train.x[3].view(28,28))\n",
    "ac2 = accuracy(model1(trainDS.x[0:1000]), trainDS.y[0:1000])\n",
    "ac3 = accuracy(model1(validDS.x[0:1000]), validDS.y[0:1000])\n",
    "#ac1, ac0, ac2, ac3\n",
    "\n",
    "x = model1(validDS.x[0:10])\n",
    "y = validDS.y[0:10]\n",
    "acc = accuracy(x,y)\n",
    "\n",
    "ac1, ac0, ac2, ac3\n",
    "#len(validDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(4)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "Going again\n"
     ]
    }
   ],
   "source": [
    "class rndprm():\n",
    "    def __init__(self):\n",
    "        self.x = torch.randperm(5)\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for dx in range(5):\n",
    "            yield self.x[dx]\n",
    "            \n",
    "x = iter(rndprm())\n",
    "for dx in x:\n",
    "    print(dx)\n",
    "    \n",
    "print(\"Going again\")\n",
    "\n",
    "for dx in x:\n",
    "    print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-be37f2029f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "d = torch.randperm(20)\n",
    "d[0:2]\n",
    "x = (1, 2)\n",
    "a, b = zip(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
