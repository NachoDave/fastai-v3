{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from expDJ.nb_4Dj import *\n",
    "from exp.nb_08 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-38fcd7b4830b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Get MNIST data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai-course-v3/nbs/dl2/exp/nb_02.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNIST_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get MNIST data\n",
    "\n",
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Function to normalize the trainig data and validation data\n",
    "def normalize_to(train, valid):\n",
    "    m,s = train.mean(),train.std()\n",
    "    return normalize(train, m, s), normalize(valid, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the x training and vaiation data\n",
    "x_train,x_valid = normalize_to(x_train,x_valid)\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the datasets\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) \n",
    "nh,bs = 50,512\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "# Make samplers\n",
    "\n",
    "train_Smp, valid_Smp = SamplerDJ(train_ds, bs, shuffle=True), SamplerDJ(valid_ds, bs, shuffle=True)\n",
    "\n",
    "# Make data loaders\n",
    "\n",
    "train_dl, valid_dl = DataLoaderDJ(train_ds, train_Smp, bs), DataLoaderDJ(valid_ds, valid_Smp, bs)\n",
    "\n",
    "# Make databunch\n",
    "db = DataBunchDJ(train_dl, valid_dl, c = c)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Reform MNIST data\n",
    "class LambdaDJ(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x): return self.func(x)\n",
    "    \n",
    "def flatten(x):      return x.view(x.shape[0], -1)\n",
    "def resize_MNIST(x): return x.view(-1, 1, 28, 28)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,resize_MNIST(x_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model(data):\n",
    "    return nn.Sequential(\n",
    "        LambdaDJ(resize_MNIST),\n",
    "        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14\n",
    "        nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7\n",
    "        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n",
    "        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        LambdaDJ(flatten),\n",
    "        nn.Linear(32,data.c)\n",
    "    )\n",
    "\n",
    "nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Module\n",
    "# Model\n",
    "model = get_cnn_model(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class earlyStop(Callback):\n",
    "    _order = 1\n",
    "    def __init__(self, stopIt = 3):\n",
    "        self.stopIt = stopIt\n",
    "        \n",
    "    #def after_batch(self):\n",
    "        #print(self.n_iter)\n",
    "     #   if self.n_iter > 10:\n",
    "       #     self.run.stop = True\n",
    "      #      return True\n",
    "        #print(self.stop) \n",
    "            \n",
    "        #pass\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        print(self.run.epoch)\n",
    "        if self.run.epoch == self.stopIt:\n",
    "            self.run.early_stop.stopIt\n",
    "            #self.stop = True\n",
    "            return True\n",
    "\n",
    "        \n",
    "class printPreds(Callback):\n",
    "    _order = 0\n",
    "    def after_batch(self):\n",
    "        print(run.pred[15])\n",
    "        print(run.yb[15])\n",
    "        \n",
    "class accuracyCheckCb(Callback):\n",
    "    _order = 1\n",
    "    \n",
    "    def __init__(self, m):\n",
    "        self.m = m\n",
    "        \n",
    "    def after_batch(self):\n",
    "        print(self.m(run.pred, run.yb))\n",
    "\n",
    "# Callbacks\n",
    "cbfs = [Recorder, partial(AvgStatsCallback,accuracy), partial(earlyStop, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = LearnerDJ(db, model, opt, loss_func)\n",
    "run = RunnerDJ(cb_funcs=cbfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%time run.fit(5, learn)\n",
    "#run.iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda\n",
    "class CudaCallback(Callback):\n",
    "    def begin_fit(self): self.model.cuda()\n",
    "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(db)\n",
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = LearnerDJ(db, model, opt, loss_func)\n",
    "cbfs = [Recorder, partial(AvgStatsCallback,accuracy),CudaCallback]\n",
    "run = RunnerDJ(cb_funcs=cbfs)\n",
    "\n",
    "%time run.fit(5, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_valid[10].view(28,28))\n",
    "pred = model.cpu()(x_valid[10:12]),y_valid,\n",
    "x_valid[10].shape, \n",
    "#loss_func(pred[0], pred[1])\n",
    "#loss_func(pred[0], pred[1][10:12])\n",
    "#pred[1][10:11]\n",
    "pred[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_means = []\n",
    "act_stds = []\n",
    "\n",
    "def append_stats(mod, inp, outp):\n",
    "    act_means.append(outp.data.mean())\n",
    "    act_stds.append(outp.data.std())\n",
    "\n",
    "model = get_cnn_model(db)\n",
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = LearnerDJ(db, model, opt, loss_func)\n",
    "cbfs = [Recorder, partial(AvgStatsCallback,accuracy),CudaCallback]\n",
    "run = RunnerDJ(cb_funcs=cbfs)\n",
    "\n",
    "model[3].register_forward_hook(append_stats)\n",
    "%time run.fit(5, learn)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(act_means),plt.plot(act_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ListContainer():\n",
    "    def __init__(self, items): self.items = listify(items)\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, (int,slice)): return self.items[idx]\n",
    "        if isinstance(idx[0],bool):\n",
    "            assert len(idx)==len(self) # bool mask\n",
    "            return [o for m,o in zip(idx,self.items) if m]\n",
    "        return [self.items[i] for i in idx]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __iter__(self): return iter(self.items)\n",
    "    def __setitem__(self, i, o): self.items[i] = o\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self)>10: res = res[:-1]+ '...]'\n",
    "        return res\n",
    "    \n",
    "class Hook():\n",
    "    def __init__(self, mLyr, func):\n",
    "        self.hook = mLyr.register_forward_hook(partial(func, self))\n",
    "        \n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[], [])\n",
    "    means,stds, meds = hook.stats\n",
    "    means.append(outp.data.mean())\n",
    "    stds.append(outp.data.std())\n",
    "    meds.append(outp.data.median())\n",
    "    \n",
    "## test \n",
    "\n",
    "model = get_cnn_model(db)\n",
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = LearnerDJ(db, model, opt, loss_func)\n",
    "cbfs = [Recorder, partial(AvgStatsCallback,accuracy),CudaCallback]\n",
    "\n",
    "\n",
    "hook = Hook(model[1], append_stats)\n",
    "\n",
    "run = RunnerDJ(cb_funcs=cbfs)\n",
    "run.fit(1, learn)\n",
    "        \n",
    "    \n",
    "from torch.nn import init\n",
    "\n",
    "class HooksDJ(ListContainer):\n",
    "    def  __init__(self, f, ms): # f is the function, m is the model\n",
    "        super().__init__([Hook(m, f) for m in ms])\n",
    "    \n",
    "    \n",
    "    def __enter__(self, *args): return self\n",
    "        \n",
    "        \n",
    "    def __exit__(self, *args): self.remove()\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.remove()\n",
    "    \n",
    "    def __delitem__(self, i):\n",
    "        self[i].remove\n",
    "        super.__delitem__(i)\n",
    "    \n",
    "    \n",
    "    def remove(self):\n",
    "        \n",
    "        for dx in self:\n",
    "            dx.remove()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(hook.stats[1])\n",
    "plt.plot(hook.stats[0])\n",
    "plt.plot(hook.stats[2])\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with Hooks container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats_hist(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[], [])\n",
    "    means,stds, hists = hook.stats\n",
    "    means.append(outp.data.mean().cpu())\n",
    "    stds.append(outp.data.std().cpu())\n",
    "    #meds.append(outp.data.median())\n",
    "    hists.append(outp.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU\n",
    "\n",
    "model = get_cnn_model(db)\n",
    "opt = optim.SGD(model.parameters(), lr=0.9)\n",
    "learn = LearnerDJ(db, model, opt, loss_func)\n",
    "cbfs = [Recorder, partial(AvgStatsCallback,accuracy),CudaCallback]\n",
    "\n",
    "with HooksDJ(append_stats_hist, model) as hooks: run.fit(1, learn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to @ste for initial version of histgram plotting code\n",
    "def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()\n",
    "\n",
    "fig,axes = plt.subplots(3,2, figsize=(15,6))\n",
    "for ax,h in zip(axes.flatten(), hooks[:6]):\n",
    "    ax.imshow(get_hist(h), origin='lower')\n",
    "    ax.axis('off')#\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooks[1] this is the layer \n",
    "len(hooks[0].stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "512*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "512*118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(db)\n",
    "modelOrig = model\n",
    "dx = 5\n",
    "print(modelOrig[dx].weight.std())\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x.sub_(self.sub)\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "\n",
    "def init_cnn(m, uniform=False):\n",
    "    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_\n",
    "    for l in m:\n",
    "        if isinstance(l, nn.Conv2d):\n",
    "            #print(\"Boom\")\n",
    "            print(\"Pre: \")\n",
    "            print(l.weight.std())\n",
    "            f(l.weight, a=0)\n",
    "            print(\"post: \")\n",
    "            print(l.weight.std())\n",
    "            l.bias.data.zero_()\n",
    "\n",
    "        \n",
    "init_cnn(model)\n",
    "\n",
    "rel = GeneralRelu(leak = 0.1)\n",
    "modelOrig[dx].weight.mean(), modelOrig[dx].weight.std()\n",
    "#model[0]\n",
    "#F.leaky_relu_(model[1].weight)\n",
    "F.leaky_relu(model[1].weight[:], 0).std(), model[1].weight.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks and stuff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
