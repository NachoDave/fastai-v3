{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_01 import *\n",
    "\n",
    "def downLoadMNIST():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "x_train,y_train,x_valid,y_valid = downLoadMNIST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([50000]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x-m)/s\n",
    "x_train = normalize(x_train, x_train.mean(), x_train.std())\n",
    "x_valid = normalize(x_valid, x_train.mean(), x_train.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linLayer(act, w, b): # act is the activations, w is the weight matrix\n",
    "    assert(act.shape[1] == w.shape[0]) # check if the no of columns in the activation matrix is the same as rows in the weigth matrix\n",
    "    return act@w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Weight matrix for first linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.1553e-05), tensor(0.0508))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of weight matrix is x_train.shape[1] x the number of hidden nodes\n",
    "# Use math.sqrt(m) for kaming init\n",
    "\n",
    "nh = 10\n",
    "m = x_train.shape[1]\n",
    "\n",
    "#w1 = torch.randn(m, nh)/math.sqrt(m) # this is for a linear layer only\n",
    "w1 = torch.randn(m, nh)*math.sqrt(2/m) # this is for linear layer ito relu \n",
    "b1 = torch.zeros(nh)\n",
    "\n",
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 10]), tensor(0.2038), tensor(1.2957))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = linLayer(x_train[0:100, :], w1, b1)\n",
    "out.shape, out.mean(), out.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Weight Matrix for second linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = torch.randn(nh, 1)/math.sqrt(m) # only one output node\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x, y):\n",
    "    return (x.squeeze_(-1) - y).pow(2).mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0946),\n",
       " tensor(0.0946),\n",
       " tensor(0.0561),\n",
       " tensor(0.2823),\n",
       " tensor(-0.0245),\n",
       " torch.Size([100, 1]),\n",
       " torch.Size([100, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = linLayer(relu(linLayer(x_train[0:100, :], w1, b1)), w2, b2)\n",
    "\n",
    "t1 = linLayer(x_train[0:100, :], w1, b1)\n",
    "t2 = relu(t1)\n",
    "t3 = linLayer(t2, w2, b2)\n",
    "\n",
    "\n",
    "t.mean(), t3.mean(), t.std(), t.max(), t.min(), t.shape, t1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26.4131)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(t, y_train[0:100])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5., 0., 4.,  ..., 8., 4., 8.]),\n",
       " tensor([ 0.2823,  0.1069,  0.0526,  0.1145,  0.1509,  0.0903,  0.0996,  0.1255,\n",
       "          0.0978,  0.1270,  0.1656,  0.0176,  0.1006,  0.0573,  0.0471,  0.0964,\n",
       "          0.1129,  0.1665,  0.0651,  0.1587, -0.0085,  0.0504,  0.1029,  0.1199,\n",
       "          0.0489,  0.1126,  0.1159,  0.1140,  0.0895,  0.0425,  0.1422,  0.0662,\n",
       "          0.0949,  0.0887,  0.1474,  0.1305,  0.1014,  0.0541,  0.0661,  0.0581,\n",
       "          0.0612,  0.2567,  0.0879,  0.1375,  0.0589,  0.1331,  0.1204,  0.2344,\n",
       "          0.0071,  0.1782,  0.1018,  0.0378,  0.0479,  0.0352,  0.1222,  0.1559,\n",
       "          0.0271,  0.1113,  0.0802,  0.1315,  0.1736,  0.0862,  0.1058,  0.0041,\n",
       "          0.0254, -0.0114,  0.1074,  0.1255,  0.0826,  0.0824,  0.1008,  0.0273,\n",
       "          0.0606,  0.0407,  0.1127,  0.0876,  0.0962,  0.1102,  0.0927,  0.0824,\n",
       "          0.0720,  0.1202,  0.0509,  0.0381,  0.1926,  0.2080,  0.0494,  0.0397,\n",
       "          0.0213,  0.0664,  0.0493,  0.0240, -0.0245,  0.0673,  0.1822,  0.1294,\n",
       "          0.0970,  0.2116,  0.1029,  0.0730]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, t.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad MSE\n",
    "def mseGrad(inp, targ):\n",
    "    # grad of loss with respect to output of previous layer\n",
    "    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]\n",
    "    \n",
    "# Grad relu\n",
    "\n",
    "def reluGrad(inp, out):\n",
    "    inp.g = (inp>0).float() * out.g\n",
    "    \n",
    "def linGrad(inp, out, w, b):\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardAndBackward(inp, targ):\n",
    "    l1 = inp @ w1 + b1 # first layer - input mat multiply by weights plus bias\n",
    "    l2 = relu(l1) # apply relu to activations\n",
    "    out = l2 @ w2 + b2 # \n",
    "    loss = mse(out, targ)\n",
    "    \n",
    "    mseGrad(out, targ)\n",
    "    linGrad(l2, out, w2, b2)\n",
    "    reluGrad(l1, l2)\n",
    "    linGrad(inp, l1, w1, b1)\n",
    "    \n",
    "    # all you need are the gradients to update the weights and biases\n",
    "    \n",
    "    #return loss\n",
    "    \n",
    "\n",
    "#f = mseGrad(t.squeeze(-1), y_train[0:100])\n",
    "#type(w1)\n",
    "#import numpy as np\n",
    "#dd = np.ndarray([1, 2, 3])\n",
    "#dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "        [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "        [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "        ...,\n",
       "        [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "        [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "        [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardAndBackward(x_train[0:100, :], y_train[0:100])\n",
    "\n",
    "\n",
    "w1.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reluGrad() missing 1 required positional argument: 'out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dda75a5568cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmseGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlinGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# where t2 and t3 are the activations either side of the linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreluGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reluGrad() missing 1 required positional argument: 'out'"
     ]
    }
   ],
   "source": [
    "mseGrad(t3, y_train[0:100])\n",
    "linGrad(t2, t3, w2, b2) # where t2 and t3 are the activations either side of the linear layer\n",
    "reluGrad(t1)\n",
    "\n",
    "t3.g.shape, w2.g.shape, w2.shape, t1.g.shape, t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]), torch.Size([100]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, y_train[0:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "<MmBackward object at 0x7f15d9c616d8>\n",
      "<bound method Tensor.backward of tensor([[8., 8.],\n",
      "        [8., 8.]], grad_fn=<MmBackward>)>\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x*2\n",
    "y = y@y\n",
    "print(y.grad_fn)\n",
    "print(y.backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch API example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class with forward and backward function\n",
    "class Module():\n",
    "    def __call__(self, *args): \n",
    "        self.args = args\n",
    "        self.out = self.forward(*args) # calls the forward function\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "\n",
    "\n",
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b # store the weight and bias tensors\n",
    "        \n",
    "    def forward(self, inp): return inp@self.w + self.b # performs calculation - result is stored in self.out \n",
    "    # (via __call__ funciton) Forward function requires an input and writes the output\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t() # inp.g not stored..\n",
    "        self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)\n",
    "        self.b.g = out.g.sum(0)\n",
    "        \n",
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)-0.5\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n",
    "    \n",
    "    \n",
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class - network connectivity\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] # layers - list of layer objects\n",
    "        self.loss = Mse() # loss - don't need loss to calculate gradients\n",
    "        \n",
    "    def __call__(self, x, targ): # loops through each layer using the output as the input to the next\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "         [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "         [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "         ...,\n",
       "         [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "         [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013],\n",
       "         [ 0.0195,  0.0640, -0.0539,  ...,  0.0348,  0.0196,  0.0013]]),\n",
       " tensor(27.2052),\n",
       " tensor([[ 7.6163e-01,  1.4848e+00,  8.2667e-01, -7.3826e-01,  3.3115e+00,\n",
       "          -5.0637e-01,  3.6960e+00, -1.2438e+00,  4.3691e-01, -1.3269e+00],\n",
       "         [-2.3715e+00, -5.9796e-01,  1.4234e+00,  9.3929e-01,  4.9834e-01,\n",
       "          -7.4829e-01,  2.5454e+00,  6.8939e-01, -7.0383e-01, -7.4188e-01],\n",
       "         [ 1.2925e+00, -2.3899e-01,  1.6978e+00,  1.1892e-01, -6.7505e-01,\n",
       "          -2.9154e+00, -1.5898e+00, -1.1483e+00,  1.0485e+00,  4.7840e-01],\n",
       "         [-9.1694e-01,  1.3774e+00, -3.6955e-01, -3.1413e-01, -1.4994e+00,\n",
       "           6.2466e-01,  1.9275e+00,  7.2208e-01,  1.0327e+00,  6.7473e-02],\n",
       "         [-1.3124e+00,  1.9904e+00,  1.9195e+00,  2.0938e+00,  1.5433e-01,\n",
       "           3.4058e-01,  1.0511e+00,  9.8616e-01, -1.9962e-01, -6.5883e-01],\n",
       "         [-2.1953e+00,  6.5927e-01,  1.5907e+00,  5.4304e-01,  9.4818e-01,\n",
       "          -7.5563e-01, -6.3648e-01,  1.0791e+00,  3.1814e+00,  1.5428e-01],\n",
       "         [-1.1380e+00,  1.5627e+00,  9.7138e-01, -1.5328e+00,  3.1062e-01,\n",
       "           2.4498e+00,  1.0372e+00,  1.2722e+00,  1.8275e+00,  3.2869e-01],\n",
       "         [-1.8324e-01, -8.8095e-01,  2.4397e+00, -5.1415e-01,  2.3261e+00,\n",
       "          -2.2055e+00,  2.9742e+00, -9.3246e-01,  1.8573e+00, -2.5518e+00],\n",
       "         [-1.5235e+00,  1.5837e+00,  8.6380e-01,  3.8530e-01, -4.0271e-01,\n",
       "           1.3855e+00,  1.7184e+00,  3.7069e-01,  6.9114e-01,  3.7289e-01],\n",
       "         [ 1.2300e-01,  4.8764e-02, -1.2042e+00,  1.4881e+00,  5.1883e-01,\n",
       "          -2.0727e+00,  1.4059e+00, -1.5111e+00,  2.0355e+00,  4.5933e-01],\n",
       "         [-3.0578e+00, -8.5089e-01,  7.4029e-01,  2.4338e-01,  3.0829e+00,\n",
       "          -1.6971e-01,  2.1695e+00, -6.3423e-01,  1.4959e+00,  7.4621e-01],\n",
       "         [-1.7627e+00, -1.5634e+00,  1.7022e+00, -7.0990e-01, -7.7315e-01,\n",
       "          -9.3147e-01,  1.2331e+00, -4.1251e-01,  1.9199e+00,  2.1816e+00],\n",
       "         [-2.1826e+00, -1.8227e+00,  4.7636e-01, -1.5039e+00,  1.5268e+00,\n",
       "           9.0103e-01,  1.7102e+00,  1.8077e-01,  1.8786e+00, -1.1934e+00],\n",
       "         [-6.3600e-01,  6.5711e-01,  6.1238e-01,  4.9960e-01, -5.7556e-01,\n",
       "           2.7435e-01, -1.0789e+00,  1.0279e+00,  1.1003e+00, -8.6417e-01],\n",
       "         [-8.8943e-01,  2.8636e-01,  6.5108e-01,  3.6215e-02,  1.7236e-01,\n",
       "           1.0681e+00,  1.4080e+00, -4.2479e-02,  1.5181e+00,  6.6591e-01],\n",
       "         [-1.5676e+00,  3.9086e-01,  6.4391e-01,  1.8494e+00,  6.1956e-01,\n",
       "          -6.8141e-01,  5.3065e-01, -7.3528e-01, -5.3873e-01, -2.0528e-01],\n",
       "         [-7.0332e-01, -6.7970e-01,  4.3268e-01,  4.2313e-01,  1.9855e+00,\n",
       "          -3.9926e-01,  1.0709e+00, -1.1222e+00,  1.7075e+00,  4.0142e-01],\n",
       "         [-8.6628e-01,  1.6117e+00, -9.5282e-01,  7.6556e-01,  5.6436e-01,\n",
       "          -4.7357e-01,  2.1971e+00,  6.2911e-01,  3.3856e-02, -4.3133e-02],\n",
       "         [-3.6968e-01,  1.2374e+00,  5.8958e-01,  1.7842e-02, -5.0021e-01,\n",
       "          -1.1822e+00,  1.1503e+00, -1.9699e-01,  9.2652e-01,  1.1062e+00],\n",
       "         [-8.5868e-01,  1.1683e+00, -5.1991e-01,  1.8726e+00, -4.5597e-01,\n",
       "          -3.8691e-02,  2.3119e+00, -7.0370e-01, -9.9671e-02,  7.7768e-02],\n",
       "         [-8.2960e-01, -1.2663e+00,  2.2636e+00,  9.8977e-02, -6.1552e-01,\n",
       "          -9.3541e-01, -6.8368e-02,  8.1723e-01,  1.1624e+00, -7.3333e-01],\n",
       "         [-2.6072e+00, -4.4996e-01,  2.8042e+00,  1.8254e+00,  7.0498e-02,\n",
       "          -2.1512e+00,  1.1823e+00,  2.1497e-01, -3.5458e-01,  7.6147e-01],\n",
       "         [-1.0666e+00, -2.3184e-01,  4.4140e-01,  1.5732e+00, -1.1252e+00,\n",
       "          -1.4071e+00,  1.9707e+00, -1.0445e+00,  8.1790e-01,  5.4442e-01],\n",
       "         [-7.6054e-01,  1.4967e+00, -4.4600e-01, -4.5189e-01, -2.0858e+00,\n",
       "           8.1421e-01,  1.9370e+00,  7.8818e-01,  1.1601e+00,  7.2066e-02],\n",
       "         [-3.7558e+00,  8.6040e-01,  1.0258e+00,  4.3886e-01, -9.4943e-01,\n",
       "          -8.5522e-01,  8.3398e-01, -1.8703e+00,  5.1691e-01,  1.3410e+00],\n",
       "         [ 3.8268e-01, -1.5941e+00,  6.8180e-01, -1.7450e+00,  2.9554e-01,\n",
       "          -2.4912e+00,  2.5813e+00,  7.7029e-01,  9.0778e-01, -1.2515e-01],\n",
       "         [-2.1886e+00,  9.6112e-03,  1.6229e+00,  2.6957e+00,  1.6299e+00,\n",
       "          -4.9996e-01, -2.1071e-02, -9.2868e-01, -6.7906e-01,  1.1333e+00],\n",
       "         [-8.5634e-02, -1.9274e+00,  2.7566e+00,  5.2136e-01,  1.6288e+00,\n",
       "          -1.2569e+00,  2.7993e+00,  5.7134e-02,  2.2823e+00, -2.0554e+00],\n",
       "         [-9.3556e-01, -2.6294e+00,  2.7116e+00, -9.4253e-02,  2.3622e+00,\n",
       "          -4.6838e-01,  1.7049e+00, -3.6766e-02,  2.0328e+00, -3.5250e-02],\n",
       "         [-8.8008e-01,  1.0244e+00,  2.1096e-01, -9.2858e-02, -1.4579e+00,\n",
       "           1.1928e-01,  5.7244e-01, -1.0268e+00,  8.8198e-02,  4.7386e-01],\n",
       "         [ 5.5693e-01,  2.1538e-01,  2.0033e+00, -4.5338e-01,  8.7059e-01,\n",
       "           2.9485e+00,  1.8665e+00,  2.7311e+00,  1.3206e+00,  7.3242e-01],\n",
       "         [-1.1288e+00, -6.4296e-01, -1.3107e-02, -1.3114e+00, -7.9744e-01,\n",
       "          -1.2318e-01,  1.6883e+00,  4.6702e-01,  1.3967e+00, -2.8714e-01],\n",
       "         [-6.5169e-01, -1.2498e-02,  1.0064e+00, -1.8100e+00, -1.3749e+00,\n",
       "          -6.4250e-01,  2.3229e+00,  1.4409e+00,  2.1959e+00, -4.9806e-01],\n",
       "         [-1.1472e+00,  1.0844e+00, -4.9149e-02, -7.5521e-01, -1.7952e-01,\n",
       "          -5.8271e-01,  1.9691e+00, -2.8806e-01,  5.0186e-01,  5.9034e-01],\n",
       "         [-1.0300e+00, -9.7825e-01, -4.9962e-01,  2.1834e-03,  2.0260e+00,\n",
       "          -1.0384e+00,  2.6393e+00, -8.3371e-01,  1.4498e+00, -6.1140e-01],\n",
       "         [-2.9585e+00,  1.8527e+00,  1.2438e+00,  1.4318e+00, -8.9674e-01,\n",
       "          -2.5581e-01,  1.6145e+00, -3.0655e-01,  8.8266e-01,  4.8556e-01],\n",
       "         [-4.3116e-01,  1.1150e+00, -6.3472e-03, -1.8098e+00,  9.4516e-02,\n",
       "           1.1291e-01,  1.8379e+00, -3.1235e-01,  2.3560e+00, -1.3456e-01],\n",
       "         [-1.9524e+00, -1.2674e+00,  1.5501e+00, -4.1748e-02,  4.5552e-01,\n",
       "          -4.3047e-01,  1.7980e+00,  4.3142e-01,  1.1656e+00, -5.2040e-01],\n",
       "         [-3.3188e-01,  3.4819e-01,  3.4201e-01,  9.5205e-01,  8.0039e-01,\n",
       "           2.1497e+00, -5.7288e-01,  7.0336e-03,  1.8559e+00,  1.5033e+00],\n",
       "         [-1.1947e+00, -7.6734e-01,  2.7076e-01, -5.9430e-01, -2.4628e-02,\n",
       "          -8.5496e-01,  1.3688e+00,  5.6025e-01,  1.8017e+00, -1.3854e+00],\n",
       "         [-1.8092e+00,  8.3141e-01,  2.7587e-01,  2.6350e-01, -2.3495e-01,\n",
       "           1.2218e+00,  9.5471e-01,  4.9249e-01, -3.2244e-01,  1.5686e+00],\n",
       "         [-2.3553e+00,  2.9950e+00, -5.2496e-01,  4.2407e-01,  1.7067e+00,\n",
       "           4.8075e-01,  3.0544e+00,  7.1783e-01, -1.7689e-01, -8.7654e-01],\n",
       "         [-1.9953e+00,  1.1459e+00,  1.6491e+00,  1.6658e+00,  8.2560e-01,\n",
       "           9.8359e-02, -2.4064e-01, -3.9749e-01, -1.9159e+00,  7.9125e-01],\n",
       "         [-2.4871e+00, -4.0189e-01,  1.1948e-02,  1.2406e+00,  5.4390e-01,\n",
       "           2.3032e-01,  2.8846e+00, -3.3593e-01,  3.0719e-01,  1.3151e+00],\n",
       "         [-1.1665e+00, -9.4187e-02,  1.3004e+00, -3.7286e-01,  1.7267e+00,\n",
       "           1.3707e+00,  6.2624e-01, -7.9769e-01,  2.0477e+00,  9.3851e-01],\n",
       "         [-2.5742e+00,  6.3974e-01,  1.6670e+00,  2.4433e+00,  1.7037e+00,\n",
       "           1.0677e+00,  4.0432e-01, -5.4641e-01, -1.7835e-01,  7.1079e-01],\n",
       "         [-1.8883e+00,  9.5899e-01,  7.3405e-01,  1.0470e+00,  3.7747e-01,\n",
       "           1.4815e-01,  1.1207e+00,  5.5499e-01,  2.0168e+00,  4.4279e-01],\n",
       "         [ 1.3166e-01,  1.2418e+00,  5.1721e-01,  6.3388e-01,  1.6260e+00,\n",
       "          -1.4162e+00,  3.4019e+00,  1.1933e+00,  6.4537e-01, -1.6006e-01],\n",
       "         [-7.4412e-02, -1.2646e+00,  1.9138e+00, -8.1644e-01,  3.6464e-01,\n",
       "           1.8719e-01,  9.0299e-01, -8.4331e-02,  8.6947e-01, -2.5064e-01],\n",
       "         [-2.0554e+00, -7.1169e-01,  1.9182e+00, -8.9405e-01,  2.7418e+00,\n",
       "          -3.7797e-01,  4.4038e+00, -1.8587e+00,  7.0040e-01, -1.7381e+00],\n",
       "         [-2.7768e+00, -8.8459e-01, -1.3461e-01, -6.3786e-01,  1.4003e+00,\n",
       "          -6.8133e-01,  1.2662e+00,  7.2136e-01,  5.1634e-01,  4.9218e-01],\n",
       "         [-9.6864e-01, -8.8055e-01,  3.7672e+00,  6.6231e-01,  7.9158e-01,\n",
       "          -1.4312e+00,  1.3739e+00,  8.1811e-01, -3.0544e-01, -1.2490e+00],\n",
       "         [ 4.0320e-02, -1.0999e+00,  3.5585e+00,  1.2794e+00,  1.9873e+00,\n",
       "          -9.2856e-01, -1.0311e+00, -6.4713e-02, -7.0430e-02,  9.3332e-01],\n",
       "         [-1.1361e+00, -2.6651e-01,  2.0434e-02,  2.8089e-01, -6.7822e-01,\n",
       "           1.7312e-01,  6.6409e-01, -8.0241e-03,  1.3379e+00, -1.1997e-01],\n",
       "         [-1.5068e-01, -1.8171e+00,  3.7521e+00,  2.5736e+00,  2.2161e+00,\n",
       "          -1.3147e+00,  1.0193e+00, -6.7593e-01,  4.2308e-01, -8.8911e-02],\n",
       "         [-1.9765e+00,  5.1007e-01,  1.0102e+00,  1.8454e+00,  1.4404e+00,\n",
       "          -1.3332e+00,  1.7963e+00, -1.4019e-01,  2.4005e-01,  2.0583e-01],\n",
       "         [-5.7888e-01, -1.0367e+00,  3.1516e+00, -3.6856e-01,  1.8829e+00,\n",
       "          -2.1343e+00,  7.8785e-01, -1.3727e-01, -1.7848e+00,  5.2634e-02],\n",
       "         [-1.9806e+00, -5.2531e-01,  2.5198e-01,  2.7842e+00, -1.2667e-01,\n",
       "          -5.5424e-01,  6.9836e-01, -7.0484e-01,  5.9589e-01,  5.4430e-01],\n",
       "         [-2.8544e-01, -3.4527e-02,  2.6860e+00,  1.8846e+00,  1.5087e+00,\n",
       "          -1.1742e+00, -1.9227e+00,  2.7278e-01,  1.1889e+00,  2.0335e-01],\n",
       "         [-1.1523e+00,  1.6132e+00, -3.5707e-01, -4.4266e-01, -9.0206e-01,\n",
       "           1.2099e+00,  2.0445e+00,  1.1620e+00,  8.7052e-01,  2.2475e-01],\n",
       "         [ 3.6300e+00, -4.4532e-02,  2.3101e+00,  4.4279e-01, -2.4145e+00,\n",
       "          -2.6135e-01, -9.4268e-01,  1.4064e-02, -1.7233e-01, -4.8806e-01],\n",
       "         [-6.8900e-01,  1.5459e+00,  4.5519e-01, -9.9259e-01, -1.1279e+00,\n",
       "          -2.0040e-01,  1.2024e+00, -5.9738e-01,  2.2663e+00, -7.1453e-01],\n",
       "         [-5.8278e-01, -7.8407e-01,  4.7559e-01,  5.3038e-01,  3.8894e-01,\n",
       "          -1.3976e+00,  1.7767e+00,  8.2873e-01,  2.2796e+00, -8.8757e-01],\n",
       "         [-2.2106e+00, -1.0819e+00,  2.2532e+00, -7.9663e-01,  4.5530e-01,\n",
       "          -1.9323e-01,  1.0997e+00, -2.4854e-01, -2.0230e-01, -3.1330e+00],\n",
       "         [-7.0334e-01, -1.1076e+00,  2.3486e+00, -6.6965e-01,  4.5887e-01,\n",
       "          -2.1619e+00, -9.8756e-01,  1.2704e+00,  2.8960e+00, -7.4382e-01],\n",
       "         [-1.2485e+00, -9.7614e-01,  1.7943e+00, -7.7811e-01, -6.1071e-01,\n",
       "          -1.5421e+00, -4.9519e-02,  7.3558e-02,  2.4028e+00,  2.3236e+00],\n",
       "         [-7.8605e-01,  4.4452e-01,  2.3415e-01,  1.3783e+00, -2.2940e-01,\n",
       "           6.4877e-01,  9.4769e-01,  7.3339e-01,  1.6833e+00,  5.0920e-01],\n",
       "         [-1.5415e+00,  1.1893e+00, -6.7411e-01,  2.0155e-01, -1.8856e+00,\n",
       "           1.1709e+00,  2.2044e+00,  1.0462e+00,  6.7261e-01, -1.4481e-01],\n",
       "         [ 1.0782e+00, -3.5487e+00, -1.9548e-01, -9.6205e-01, -2.1247e-01,\n",
       "          -2.8261e+00,  8.3567e-01, -1.4656e+00, -7.9679e-01,  1.5185e+00],\n",
       "         [-1.4044e+00, -3.2287e+00,  2.1947e+00, -8.8630e-02,  1.9701e+00,\n",
       "          -2.2576e+00,  7.2771e-01,  7.6103e-01,  2.5572e+00, -7.2668e-01],\n",
       "         [-6.1525e-01,  3.9784e-01,  6.3721e-01, -1.0979e+00,  4.3434e-01,\n",
       "           1.1096e+00,  2.4316e+00,  7.4869e-01,  1.1946e+00,  2.4710e-02],\n",
       "         [-1.5424e+00,  3.0501e-01,  2.6891e-01, -1.2269e+00, -4.6060e-01,\n",
       "           1.0196e+00,  1.0595e+00, -1.0911e+00, -1.9515e-01, -5.9433e-01],\n",
       "         [-9.2014e-01,  3.6614e-01,  2.3599e-01, -2.5827e-01, -1.0314e+00,\n",
       "           1.4240e+00,  1.4788e+00,  7.3093e-01,  4.6365e-01,  7.0802e-01],\n",
       "         [-7.8624e-01,  3.4550e-01,  6.0643e-01,  6.2306e-01, -2.3832e+00,\n",
       "          -1.3925e+00,  2.6011e-01,  3.3045e-01,  6.9080e-01,  6.3203e-01],\n",
       "         [-1.4421e+00, -1.0007e+00,  1.4393e+00, -9.0946e-01,  2.3525e+00,\n",
       "           2.3153e-01,  2.0351e+00, -1.1842e+00,  1.0813e+00, -1.2487e+00],\n",
       "         [-1.7970e+00, -2.0950e+00,  8.5047e-01, -1.3237e-01,  9.7585e-01,\n",
       "          -5.6423e-01,  2.2561e+00, -9.7497e-01,  1.4997e+00, -1.7704e-01],\n",
       "         [-6.2263e-01,  7.2269e-01, -4.4579e-01, -2.5540e+00,  5.9937e-01,\n",
       "           2.4850e+00,  2.4938e+00, -3.0370e-02,  1.0498e-01,  5.5698e-01],\n",
       "         [-1.4905e+00,  8.4974e-01, -6.5899e-01, -7.2653e-01, -1.7487e-01,\n",
       "           1.6554e+00,  2.5301e+00,  7.9377e-01,  8.1752e-01,  8.2081e-01],\n",
       "         [-2.1316e+00,  3.8972e-01, -3.6079e-01,  6.9615e-01, -5.8529e-01,\n",
       "           7.4307e-01,  1.9674e+00,  9.7438e-02,  8.7647e-01,  1.1702e+00],\n",
       "         [-9.8077e-01,  1.6710e+00,  1.7482e-01, -1.0359e+00, -6.9383e-01,\n",
       "           3.1322e-01,  1.3967e+00, -4.3801e-01, -5.7249e-01, -2.9567e-01],\n",
       "         [ 3.6914e-01, -1.3676e+00,  5.4265e-01, -5.8830e-01, -2.6998e-01,\n",
       "          -1.4816e+00,  1.4894e+00,  7.7792e-01,  3.7299e-01, -1.5220e+00],\n",
       "         [-8.8659e-01, -2.0725e+00,  1.4441e+00, -5.9331e-01,  3.1209e+00,\n",
       "          -1.4907e+00,  1.3851e+00, -9.6271e-02,  8.3595e-01, -1.1099e+00],\n",
       "         [-6.9027e-01, -1.3911e+00,  1.3097e+00, -1.1988e+00,  2.6403e-01,\n",
       "          -2.3743e+00,  1.6544e+00, -3.5517e-01,  3.0463e+00, -2.4700e+00],\n",
       "         [-1.6679e+00,  3.9668e-01,  1.5730e+00,  8.3946e-01, -2.4925e+00,\n",
       "          -8.4191e-01,  7.5794e-01, -9.8594e-02,  8.6900e-01,  1.7666e-02],\n",
       "         [-1.1156e+00,  8.4382e-03,  1.2586e+00,  1.6197e+00,  3.5577e+00,\n",
       "          -1.6022e+00,  1.2639e+00, -1.8613e+00,  1.3386e+00,  6.4114e-02],\n",
       "         [-2.9929e+00,  2.0310e+00,  1.0951e-01, -1.2293e+00,  1.6495e+00,\n",
       "          -1.2465e-01,  3.4618e+00, -6.4149e-01,  3.7974e-01, -1.8082e+00],\n",
       "         [-1.2513e+00, -8.5750e-01,  9.9448e-01, -1.0798e+00, -8.9986e-01,\n",
       "          -6.4302e-01,  1.8557e+00,  3.7529e-01,  1.1071e+00,  1.1191e+00],\n",
       "         [-1.4508e+00,  2.0976e-01,  1.2683e+00,  1.6816e+00, -1.8842e-01,\n",
       "          -3.8279e-01, -5.4566e-01, -1.5530e-01, -2.5619e-01, -4.7612e-02],\n",
       "         [-6.8967e-01, -6.5766e-01,  1.0960e+00, -1.1284e+00,  3.5264e-01,\n",
       "          -9.9305e-01,  1.0986e+00, -8.5311e-01, -2.4145e-01, -1.1637e+00],\n",
       "         [-3.3807e-01, -3.6651e-01,  6.3457e-01,  1.3084e+00, -2.1080e-01,\n",
       "          -1.5477e+00,  1.0261e+00, -6.0877e-02,  9.7860e-01, -1.2307e+00],\n",
       "         [-9.2411e-01,  8.1423e-01,  9.1403e-01,  7.7693e-01, -1.5855e+00,\n",
       "           1.3738e+00,  3.9596e-01,  4.0848e-01,  2.3772e-01, -1.7508e+00],\n",
       "         [-1.6957e+00, -4.2249e-01,  1.2939e+00,  3.1859e-01,  1.0844e+00,\n",
       "           4.8527e-01, -6.3661e-02, -1.8739e+00,  4.1608e-01, -1.5224e+00],\n",
       "         [-9.6247e-01,  2.0897e-01,  2.1772e+00, -2.9360e-01, -6.3443e-01,\n",
       "          -3.7625e-01, -1.6328e+00, -6.6508e-01,  1.0997e+00,  2.8479e-01],\n",
       "         [-1.0784e+00,  4.7077e-01,  1.5662e+00,  3.2551e-01, -8.0785e-01,\n",
       "          -9.9078e-01,  3.2874e-01,  2.0328e+00,  1.5750e+00, -1.8167e-01],\n",
       "         [-1.1712e+00,  7.4641e-01,  1.0461e+00,  5.3596e-01,  2.4409e+00,\n",
       "          -4.0508e-01,  2.9717e+00,  4.2593e-02,  6.9749e-03, -2.2294e-01],\n",
       "         [-1.5392e+00, -2.0277e+00,  1.2742e+00, -5.0238e-01,  2.6205e+00,\n",
       "          -1.4309e+00,  2.2693e+00, -1.4541e-01,  7.3063e-01, -2.2737e+00],\n",
       "         [-2.2637e+00,  5.1593e-01,  1.3145e+00,  2.0890e+00,  8.2787e-01,\n",
       "           4.4053e-01,  3.5594e-01, -6.8033e-01, -9.1324e-01,  1.0623e+00],\n",
       "         [-1.6382e+00,  1.0427e+00,  4.6595e-02,  5.8977e-01,  2.1290e+00,\n",
       "          -9.7078e-01,  3.3769e+00, -3.7295e-01,  2.1989e-01, -4.6043e-01],\n",
       "         [-1.3087e+00, -1.6053e+00,  9.0554e-01, -1.3532e+00,  1.8225e+00,\n",
       "           3.2322e-02,  2.0676e+00, -1.2228e+00,  5.6711e-01,  1.4114e-01],\n",
       "         [-1.8287e+00,  6.2071e-01,  1.1964e-02, -4.0845e-01, -1.5883e-01,\n",
       "          -1.8511e-01,  1.6991e+00,  5.8936e-02,  1.0355e+00,  1.0966e+00]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f = Lin(w1, b1)\n",
    "#f(x_train) # forward layer\n",
    "\n",
    "#f.backward, f.bwd\n",
    "\n",
    "model = Model()\n",
    "model(x_train[0:100, :], y_train[0:100])\n",
    "\n",
    "model.layers[0].w.g, model.loss.out, model.layers[0].out\n",
    "\n",
    "#f.backwar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
