
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/4Dj.ipynb

#export
from exp.nb_02 import *
import torch.nn.functional as F # all nn functions
import torch.nn as nn

#export
from exp.nb_03 import *

# Sampler class
class SamplerDJ():
    def __init__(self, ds, bs, shuffle = False):
        self.n, self.bs, self.shuffle = len(ds), bs, shuffle

    def __iter__(self):
        if self.shuffle:
            self.dx = torch.randperm(self.n)
        else:
            self.dx = torch.arange(self.n)
        for idx in range(0, self.n, self.bs):
            yield self.dx[idx:idx + self.bs]

# Collate function
def collateDJ(b):
    xs,ys = zip(*b)
    return torch.stack(xs),torch.stack(ys)

class DataLoaderDJ():
    def __init__(self, ds, smp, bs, collate = collateDJ):
        self.ds, self.smp, self.bs, self.collate = ds, smp, bs, collate

    def __iter__(self): # returns a data batch
        for sdx in self.smp:
            yield self.collate(([self.ds[i] for i in sdx]))

    def __len__(self): # returns number of batches
            return len(self.ds)//self.bs


class DataBunchDJ():
    def __init__(self, train_dl, valid_dl, c=None):
        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c # c is final number of acitvations

    @property
    def train_ds(self): return self.train_dl.dataset

    @property
    def valid_ds(self): return self.valid_dl.dataset

class LearnerDJ():
    def __init__(self, db, model, opt, lossFunc):
        self.db, self.model, self.opt, self.lossFunc = db, model, opt, lossFunc


class ModelDJ(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.layers = nn.ModuleList(layers)

    def __call__(self, x):
        for l in self.layers: x = l(x)
        return x

class OptimizerDJ():
    def __init__(self, params, lr): # params are from the model
        self.lr, self.params = lr, list(params)

    def step(self): # step through back ward pass
        with torch.no_grad():
            for dx in self.params:
                dx -= dx.grad*self.lr

    def zero_grad(self): # zero gradient
        for dx in self.params:
            dx.grad.data.zero_()



import re

_camel_re1 = re.compile('(.)([A-Z][a-z]+)')
_camel_re2 = re.compile('([a-z0-9])([A-Z])')
def camel2snake(name):
    s1 = re.sub(_camel_re1, r'\1_\2', name)
    return re.sub(_camel_re2, r'\1_\2', s1).lower()

class Callback():
    _order=0
    def set_runner(self, run): self.run=run
    def __getattr__(self, k): return getattr(self.run, k) # gets the object from self.run
    @property
    def name(self):
        name = re.sub(r'Callback$', '', self.__class__.__name__)
        return camel2snake(name or 'callback')



class TrainEvalCallback(Callback):
    def begin_fit(self):
        self.run.n_epochs=0.
        self.run.n_iter=0

    def after_batch(self):
        if not self.in_train: return
        self.run.n_epochs += 1./self.iters
        self.run.n_iter   += 1
        print("Iter: " + str(self.run.n_iter))


    def begin_epoch(self):
        self.run.n_epochs=self.epoch
        self.model.train()
        self.run.in_train=True
        print("Epoch: " + str(self.run.epochs))
        self.run.n_iter = 0

    def begin_validate(self):
        self.model.eval()
        self.run.in_train=False




from typing import *

def listify(o): # converts an iterator to a list
    if o is None: return []
    if isinstance(o, list): return o
    if isinstance(o, str): return [o]
    if isinstance(o, Iterable): return list(o)
    return [o]


#

# runner class
class RunnerDJ():
    # init takes in call backs, assigns to a list
    def __init__(self, cbs=None, cb_funcs=None):
        cbs = listify(cbs) # make a list of the call backs
        #print(cbs)s
        #print(cb_funcs)
        for cbf in listify(cb_funcs):
            cb = cbf()
            print(cb.name)
            setattr(self, cb.name, cb)
            cbs.append(cb)
        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs

    # These are the learner members, allows self. to be used inside the runner to access them
    @property
    def opt(self): return self.learn.opt
    @property
    def model(self): return self.learn.model
    @property
    def loss_func(self): return self.learn.lossFunc
    @property
    def data(self): return self.learn.db

    def one_batch(self, xb, yb):
        self.xb, self.yb = xb, yb
        if self('begin_batch'): return # if call back function returns TRUE then return from method
        # training
        self.pred = self.model(self.xb) # predictions
        if self('after_pred'): return
        self.loss = self.loss_func(self.pred, self.yb) # loss function
        #print(self.loss)
        if self('after_loss') or not self.in_train: return
        # Backward pass
        self.loss.backward() # backwad function
        if self('after_backward'): return
        self.opt.step() #
        if self('after_step') : return
        self.opt.zero_grad() # zero gradients



        # valiation

    def all_batches(self, dl):
        self.iters = len(dl)
        for xb, yb in dl:
            if self.stop: break
            self.one_batch(xb, yb)
            self('after_batch')
        self.stop = False

    def fit(self, epochs, learn):
        self.epochs,self.learn = epochs,learn

        try:
            for cb in self.cbs: cb.set_runner(self) # loops through call backs and tells them what learner they're in

            # If you need a runner member varible then you can get it in the callback
            if self('begin_fit'): return
            for epoch in range(epochs):
                self.epoch = epoch
                if not self('begin_epoch'): self.all_batches(self.data.train_dl) # forward pass run if begin_epoch doesn't return 0

                with torch.no_grad():
                    if not self('begin_validate'): self.all_batches(self.data.valid_dl) # backward pass
                if self('after_epoch'): break

        finally:
            self('after_fit')
            self.learn = None

    def __call__(self, cb_name): # calls the call back
        #print(cb_name)
        for cb in sorted(self.cbs, key=lambda x: x._order): # sorts call backs by order
            f = getattr(cb, cb_name, None) # if the cb has a cb_name function then get it
            if f and f(): return True
        return False


class AvgStats():
    def __init__(self, metrics, in_train):
        self.metrics,self.in_train = listify(metrics),in_train
        #print("Metrics:")
        #print(metrics)
    def reset(self):
        self.tot_loss,self.count = 0.,0
        self.tot_mets = [0.] * len(self.metrics)

    @property
    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets
    @property
    def avg_stats(self): return [o/self.count for o in self.all_stats]

    def __repr__(self):
        if not self.count: return ""
        return f"{'train' if self.in_train else 'valid'}: {self.avg_stats}"

    def accumulate(self, run):
        bn = run.xb.shape[0]
        self.tot_loss += run.loss * bn
        self.count += bn
        for i,m in enumerate(self.metrics):
            self.tot_mets[i] += m(run.pred, run.yb) * bn # can have any metrics on the predictions and truth

class AvgStatsCallback(Callback):
    def __init__(self, metrics):
        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)

    def begin_epoch(self):
        self.train_stats.reset()
        self.valid_stats.reset()

    def after_loss(self):
        stats = self.train_stats if self.in_train else self.valid_stats
        with torch.no_grad():
            stats.accumulate(self.run) # passes the runner in
            #print(self.run)

    def after_epoch(self):
        print(self.train_stats)
        print(self.valid_stats)


# Call back funcs generate call backs from a function
from functools import partial

acc_cbf = partial(AvgStatsCallback,accuracy) # AvgStatsCallback constructor, pass in accuracy function

# Recorder class -
class Recorder(Callback):
    def begin_fit(self): self.lrs,self.losses = [],[]

    def after_batch(self):
        if not self.in_train: return
        self.lrs.append(self.opt.param_groups[-1]['lr'])
        self.losses.append(self.loss.detach().cpu())

    def plot_lr  (self): plt.plot(self.lrs)
    def plot_loss(self): plt.plot(self.losses)



rcd_cbf = Recorder

class ParamScheduler(Callback):
    _order=1 # pname is the hyper parameter name
    def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func

    def set_param(self):
        for pg in self.opt.param_groups:
            pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)

    def begin_batch(self):
        if self.in_train: self.set_param()

#export
def annealer(f):
    def _inner(start, end): return partial(f, start, end)
    return _inner

@annealer
def sched_lin(start, end, pos): return start + pos*(end-start)

f = sched_lin(1,2)
f(0.4)

@annealer
def sched_sin(start, end , pos): return math.sin(math.pi*(pos))
ff = sched_sin(0, 1)

@annealer
def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2
@annealer
def sched_no(start, end, pos):  return start
@annealer
def sched_exp(start, end, pos): return start * (end/start) ** pos

#This monkey-patch is there to be able to plot tensors
torch.Tensor.ndim = property(lambda x: len(x.shape))

# Combine schedulers
def combine_scheds(pcts, scheds):
    assert sum(pcts) == 1.
    pcts = tensor([0] + listify(pcts))
    assert torch.all(pcts >= 0)
    pcts = torch.cumsum(pcts, 0)
    def _inner(pos):
        idx = (pos >= pcts).nonzero().max()
        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])
        return scheds[idx](actual_pos)
    return _inner

